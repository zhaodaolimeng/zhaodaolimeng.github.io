
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>LDA模型入门</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="stylesheet" type="text/css" href="../assets/built/screen.css?v=de5b81e416">

    <link rel="shortcut icon" href="../favicon.ico" type="image/x-icon">
    <link rel="canonical" href="index.html">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <link rel="amphtml" href="amp/index.html">
    
    <meta property="og:site_name" content="愚人拙记">
    <meta property="og:type" content="article">
    <meta property="og:title" content="LDA模型入门">
    <meta property="og:description" content="1 引子 本文是对Blei等人LDA原始论文的总结。给定大量的文档，如何在无标注的情况下确定每个文档的主题词？LDA(Latent Dirichlet Allocation)是这类主题确定问题的一个成熟的解决方案。LDA最初面向文本挖掘领域，但随后在图像分类、行为识别等领域也得到了应用。LDA是一种典型的非监督模型，模型仅需要输入文档集合的词袋模型，模型可输出每个文档对应的主题，每个主题使用关键词的分布来表示。 2 模型定义 LDA的PGM形式如上，我们认为主题数目有K个，文档有M个， 每个文档中有N个词。其中，$\alpha$ 是Dirichlet分布的参数，大小为1xK，用于控制生成主题的聚集程度； $\theta$ 表示一个文档中主题的分布大小为1xK； $z$ 为一个为每个词安排主题的01随机变量，大小为1xK，且只有一个值为1； $\beta$ 为一个多项分布的集合，大小为KxV，其中每一行代表一个主题中，不同词出现的概率；而w代表每个文档中的一个词。 沿着上面的PGM的箭头方向，可以总结出词的生成过程。我们已知了每个文档中的词袋模型w，为了找到一组合适的主题，需要对分布 $p(w\vert\alpha,\beta)">
    <meta property="og:url" content="http://localhost:2368/ldamo-xing-ru-men/">
    <meta property="article:published_time" content="2016-08-18T12:52:00.000Z">
    <meta property="article:modified_time" content="2017-09-03T12:22:26.000Z">
    <meta property="article:tag" content="机器学习">
    
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="LDA模型入门">
    <meta name="twitter:description" content="1 引子 本文是对Blei等人LDA原始论文的总结。给定大量的文档，如何在无标注的情况下确定每个文档的主题词？LDA(Latent Dirichlet Allocation)是这类主题确定问题的一个成熟的解决方案。LDA最初面向文本挖掘领域，但随后在图像分类、行为识别等领域也得到了应用。LDA是一种典型的非监督模型，模型仅需要输入文档集合的词袋模型，模型可输出每个文档对应的主题，每个主题使用关键词的分布来表示。 2 模型定义 LDA的PGM形式如上，我们认为主题数目有K个，文档有M个， 每个文档中有N个词。其中，$\alpha$ 是Dirichlet分布的参数，大小为1xK，用于控制生成主题的聚集程度； $\theta$ 表示一个文档中主题的分布大小为1xK； $z$ 为一个为每个词安排主题的01随机变量，大小为1xK，且只有一个值为1； $\beta$ 为一个多项分布的集合，大小为KxV，其中每一行代表一个主题中，不同词出现的概率；而w代表每个文档中的一个词。 沿着上面的PGM的箭头方向，可以总结出词的生成过程。我们已知了每个文档中的词袋模型w，为了找到一组合适的主题，需要对分布 $p(w\vert\alpha,\beta)">
    <meta name="twitter:url" content="http://localhost:2368/ldamo-xing-ru-men/">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="zhaodaolimeng">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="机器学习">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "愚人拙记",
        "logo": "https://casper.ghost.org/v1.0.0/images/ghost-logo.svg"
    },
    "author": {
        "@type": "Person",
        "name": "zhaodaolimeng",
        "image": {
            "@type": "ImageObject",
            "url": "//www.gravatar.com/avatar/731e3411d9e4ccc30af1dcd7c4c5af9f?s=250&d=mm&r=x",
            "width": 250,
            "height": 250
        },
        "url": "http://localhost:2368/author/zhaodaolimeng/",
        "sameAs": []
    },
    "headline": "LDA模型入门",
    "url": "http://localhost:2368/ldamo-xing-ru-men/",
    "datePublished": "2016-08-18T12:52:00.000Z",
    "dateModified": "2017-09-03T12:22:26.000Z",
    "keywords": "机器学习",
    "description": "1 引子 本文是对Blei等人LDA原始论文的总结。给定大量的文档，如何在无标注的情况下确定每个文档的主题词？LDA(Latent Dirichlet Allocation)是这类主题确定问题的一个成熟的解决方案。LDA最初面向文本挖掘领域，但随后在图像分类、行为识别等领域也得到了应用。LDA是一种典型的非监督模型，模型仅需要输入文档集合的词袋模型，模型可输出每个文档对应的主题，每个主题使用关键词的分布来表示。 2 模型定义 LDA的PGM形式如上，我们认为主题数目有K个，文档有M个， 每个文档中有N个词。其中，$\\alpha$ 是Dirichlet分布的参数，大小为1xK，用于控制生成主题的聚集程度； $\\theta$ 表示一个文档中主题的分布大小为1xK； $z$ 为一个为每个词安排主题的01随机变量，大小为1xK，且只有一个值为1； $\\beta$ 为一个多项分布的集合，大小为KxV，其中每一行代表一个主题中，不同词出现的概率；而w代表每个文档中的一个词。 沿着上面的PGM的箭头方向，可以总结出词的生成过程。我们已知了每个文档中的词袋模型w，为了找到一组合适的主题，需要对分布 $p(w\\vert\\alpha,\\beta)",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:2368/"
    }
}
    </script>

    <script type="text/javascript" src="../public/ghost-sdk.js?v=de5b81e416"></script>
<script type="text/javascript">
ghost.init({
	clientId: "ghost-frontend",
	clientSecret: "934a083a6285"
});
</script>
    <meta name="generator" content="Ghost 1.7">
    <link rel="alternate" type="application/rss+xml" title="愚人拙记" href="../rss/index.html">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [["$", "$"], ["\\(", "\\)"]],
        processEscapes: true
    }});
</script>

</head>
<body class="post-template tag-ji-qi-xue-xi">

    <div class="site-wrapper">

        

<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
                <a class="site-nav-logo" href="../"><img src="https://casper.ghost.org/v1.0.0/images/ghost-logo.svg" alt="愚人拙记"></a>
            <ul class="nav">
    <li class="nav-home" role="presentation"><a href="../">Home</a></li>
</ul>
    </div>
    <div class="site-nav-right">
        <div class="social-links">
        </div>
            <a class="rss-button" href="http://cloud.feedly.com/#subscription/feed/http://localhost:2368/rss/" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24"><circle cx="6.18" cy="17.82" r="2.18"></circle><path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"></path></svg>
</a>
    </div>
</nav>
    </div>
</header>


<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full post tag-ji-qi-xue-xi no-image">

            <header class="post-full-header">
                <section class="post-full-meta">
                    <time class="post-full-meta-date" datetime="2016-08-18">18 August 2016</time>
                        <span class="date-divider">/</span> <a href="../tag/ji-qi-xue-xi/">机器学习</a>
                </section>
                <h1 class="post-full-title">LDA模型入门</h1>
            </header>


            <section class="post-full-content">
                <div class="kg-card-markdown"><h1 id="1">1 引子</h1>
<p>本文是对Blei等人LDA原始论文的总结。给定大量的文档，如何在无标注的情况下确定每个文档的主题词？LDA(Latent Dirichlet Allocation)是这类主题确定问题的一个成熟的解决方案。LDA最初面向文本挖掘领域，但随后在图像分类、行为识别等领域也得到了应用。LDA是一种典型的非监督模型，模型仅需要输入文档集合的词袋模型，模型可输出每个文档对应的主题，每个主题使用关键词的分布来表示。</p>
<h1 id="2">2 模型定义</h1>
<p><img src="../content/images/2017/09/lda.png" alt="lda"></p>
<p>LDA的PGM形式如上，我们认为主题数目有K个，文档有M个， 每个文档中有N个词。其中，$\alpha$ 是Dirichlet分布的参数，大小为1xK，用于控制生成主题的聚集程度； $\theta$ 表示一个文档中主题的分布大小为1xK； $z$ 为一个为每个词安排主题的01随机变量，大小为1xK，且只有一个值为1； $\beta$ 为一个多项分布的集合，大小为KxV，其中每一行代表一个主题中，不同词出现的概率；而w代表每个文档中的一个词。</p>
<p>沿着上面的PGM的箭头方向，可以总结出词的生成过程。我们已知了每个文档中的词袋模型w，为了找到一组合适的主题，需要对分布 $p(w\vert\alpha,\beta)$ 进行推理。由于该分部中蕴含了隐变量主题 $\theta$ ，所以积分将 $\theta$ 积掉。代入Dirichlet分布 $p(\theta\vert\alpha)$ ，多项分布 $p(z_n\vert\theta)$ ，以及一个单独的概率值 $p(w_n\vert z_n,\beta)$ ，可得参数的后验概率形式。以下为完整的推导：</p>
<p>$$p(w|\alpha,\beta) = \int p(\theta|\alpha)\prod_{n=1}^N p(w|\theta, \beta) d\theta$$<br>
$$= \int p(\theta|\alpha) (\prod_{n=1}^N \sum_{z_n}p(z_n|\theta)p(w_n|z_n,\beta))$$<br>
$$ = \frac{\Gamma(\sum_i\alpha_i)}{\prod_i{\Gamma(\alpha_i)}}\int(\prod_{i=1}^k\theta_i^{\alpha_i-1})(\prod_{n=1}^N\sum_{i=1}^k\prod_{j=1}^V(\theta_i\beta_{ij})^{w_n^j})d\theta$$</p>
<p>模型的两个关键参数可以通过多种方法进行求解，即模型训练。</p>
<h1 id="3">3 模型训练</h1>
<h2 id="31">3.1 变分推理</h2>
<p>Blei最初的LDA论文中，使用了变分推理（VB）求解LDA参数。这种方法试图使用一个不受约束的变分分布近似LDA的模型的联合概率。类似的手段可以参见Laplace近似，最经典的应用为使用高斯分布近似Bayesian Logistic Regression中观测的后验分布 $p(w\vert\bf{t})$ 。VB个人理解为一种链式的迭代估计框架。使用一个Q函数去近似真实分布函数。</p>
<h2 id="32gibbssampling">3.2 Gibbs Sampling</h2>
<p>优势是便于编程实现。</p>
<h2 id="33">3.3 比较</h2>
<p>变分推理的计算快于基于采样的方法，但可能会收敛到局部最优解。Matthew、Blei等人对于<a href="http://papers.nips.cc/paper/3902-online-learning-for-latentdirichlet-allocation!">LDA在线学习</a><br>
中对变分推理进行了改进。采样方法更为直观、易于工程实现，且在多数场景下，采样的最终性能会好于变分推理。</p>
<h1 id="4">4 参考文献</h1>
<p><a href="http://dl.acm.org/citation.cfm?id=944919.944937">Blei, David. Latent Dirichlet Allocation</a></p>
</div>
            </section>


            <footer class="post-full-footer">

                <section class="author-card">
                        <img class="author-profile-image" src="http://www.gravatar.com/avatar/731e3411d9e4ccc30af1dcd7c4c5af9f?s=250&amp;d=mm&amp;r=x" alt="zhaodaolimeng">
                    <section class="author-card-content">
                        <h4 class="author-card-name"><a href="../author/zhaodaolimeng/">zhaodaolimeng</a></h4>
                            <p>Read <a href="../author/zhaodaolimeng/">more posts</a> by this author.</p>
                    </section>
                </section>
                <div class="post-full-footer-right">
                    <a class="author-card-button" href="../author/zhaodaolimeng/">Read More</a>
                </div>

            </footer>


        </article>

        <!-- UY BEGIN -->
        <div id="uyan_frame"></div>
        <script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=2143114"></script>
        <!-- UY END -->

    </div>
</main>

<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">

                <article class="read-next-card" style="background-image: url(https://casper.ghost.org/v1.0.0/images/blog-cover.jpg)">
                    <header class="read-next-card-header">
                        <small class="read-next-card-header-sitetitle">— 愚人拙记 —</small>
                        <h3 class="read-next-card-header-title"><a href="../tag/ji-qi-xue-xi/">机器学习</a></h3>
                    </header>
                    <div class="read-next-divider"><svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24"><path d="M13 14.5s2 3 5 3 5.5-2.463 5.5-5.5S21 6.5 18 6.5c-5 0-7 11-12 11C2.962 17.5.5 15.037.5 12S3 6.5 6 6.5s4.5 3.5 4.5 3.5"></path></svg>
</div>
                    <div class="read-next-card-content">
                        <ul>
                            <li><a href="../snorkelxue-xi-bi-ji/">Snorkel学习笔记</a></li>
                            <li><a href="../deepdivexue-xi-bi-ji/">Deepdive学习笔记</a></li>
                            <li><a href="../prml-ch10du-shu-bi-ji-bian-fen-tui-li/">PRML.Ch10读书笔记：变分推理</a></li>
                        </ul>
                    </div>
                    <footer class="read-next-card-footer">
                        <a href="../tag/ji-qi-xue-xi/">See all 4 posts →</a>
                    </footer>
                </article>

                <article class="post-card post tag-ji-qi-xue-xi tag-zhi-shi-chou-qu no-image">
    <div class="post-card-content">
        <a class="post-card-content-link" href="../deepdivexue-xi-bi-ji/">
            <header class="post-card-header">
                    <span class="post-card-tags">机器学习</span>
                <h2 class="post-card-title">Deepdive学习笔记</h2>
            </header>
            <section class="post-card-excerpt">
                <p>0 简介 deepdive是一个具有语言识别能力的信息抽取工具，可用作KBC系统（Knowledge Base Construction）的内核。 也可以理解为是一种Automatic KBC工具。 由于基于语法分析器构建，所以deepdive可通过各类文本规则实现实体间关系的抽取。 deepdive面向异构、海量数据，所以其中涉及一些增量处理的机制。 PaleoDeepdive是基于deepdive的一个例子，用于推测人、地点、组织之间的关系。 deepdive的执行过程可以分为：feature extraction，probabilistic knowledge engineering，statistical inference and learning三部分。系统结构图如下所示： KBC系统中的四个主要概念： Entity Relationship</p>
            </section>
        </a>
        <footer class="post-card-meta">
                <img class="author-profile-image" src="http://www.gravatar.com/avatar/731e3411d9e4ccc30af1dcd7c4c5af9f?s=250&amp;d=mm&amp;r=x" alt="zhaodaolimeng">
            <span class="post-card-author"><a href="../author/zhaodaolimeng/">zhaodaolimeng</a></span>
        </footer>
    </div>
</article>

                <article class="post-card post tag-ji-qi-xue-xi tag-du-shu-bi-ji no-image">
    <div class="post-card-content">
        <a class="post-card-content-link" href="../prml-ch10du-shu-bi-ji-bian-fen-tui-li/">
            <header class="post-card-header">
                    <span class="post-card-tags">机器学习</span>
                <h2 class="post-card-title">PRML.Ch10读书笔记：变分推理</h2>
            </header>
            <section class="post-card-excerpt">
                <p>0 疑问 这类概率推理问题在没有VB方法的时候都是怎么求解的？ VB的直接好处是什么？ 什么是平均场估计？ 这里的估计方法和概率图中的BP的具体关系？ VB中每一步的模型都是设定好的吗？例如LDA中使用Dirichlet作为后验概率？ LDA中的VB是如何推导的？ 1 引子 本文是PRML第10章部分内容的摘录和总结。在很多概率问题中，如果使用精确求解，那么问题规模与随机变量的个数是指数上升的。以主题模型LDA为例，每个词的生成对应一个随机变量，使用确定性的方法会导致问题规模为$K^{NM}$。现有的估计方法包括变分推导、随机模拟/采样、MCMC方法。其中变分推理是一个实现框架，具体而言有Loopy belief propagation方法和Mean field approximation方法。为了简单，以下VB即是说变分推理。 用最简单的话来讲，</p>
            </section>
        </a>
        <footer class="post-card-meta">
                <img class="author-profile-image" src="http://www.gravatar.com/avatar/731e3411d9e4ccc30af1dcd7c4c5af9f?s=250&amp;d=mm&amp;r=x" alt="zhaodaolimeng">
            <span class="post-card-author"><a href="../author/zhaodaolimeng/">zhaodaolimeng</a></span>
        </footer>
    </div>
</article>

        </div>
    </div>
</aside>

<div class="floating-header">
    <div class="floating-header-logo">
        <a href="../">
            <span>愚人拙记</span>
        </a>
    </div>
    <span class="floating-header-divider">—</span>
    <div class="floating-header-title">LDA模型入门</div>
    <div class="floating-header-share">
        <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
    <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"></path>
</svg>
</div>
        <a class="floating-header-share-tw" href="https://twitter.com/share?text=LDA%E6%A8%A1%E5%9E%8B%E5%85%A5%E9%97%A8&amp;url=http://localhost:2368/ldamo-xing-ru-men/" onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"></path></svg>
        </a>
        <a class="floating-header-share-fb" href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:2368/ldamo-xing-ru-men/" onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"></path></svg>
        </a>
    </div>
    <progress class="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>
</div>




        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="../">愚人拙记</a> © 2017</section>
                <nav class="site-footer-nav">
                    <a href="../">Latest Posts</a>
                    
                    
                    <a href="https://ghost.org" target="_blank">Ghost</a>
                </nav>
            </div>
        </footer>

    </div>


    <script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="../assets/js/jquery.fitvids.js?v=de5b81e416"></script>


    <script>

// NOTE: Scroll performance is poor in Safari
// - this appears to be due to the events firing much more slowly in Safari.
//   Dropping the scroll event and using only a raf loop results in smoother
//   scrolling but continuous processing even when not scrolling
$(document).ready(function () {
    // Start fitVids
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    // End fitVids

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        // show/hide floating header
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>


    

</body>
