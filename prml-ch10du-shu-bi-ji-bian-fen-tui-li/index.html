<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>PRML.Ch10读书笔记：变分推理</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="stylesheet" type="text/css" href="../assets/built/screen.css?v=0be4e97106">

    <link rel="shortcut icon" href="../favicon.ico" type="image/x-icon">
    <link rel="canonical" href="index.html">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <link rel="amphtml" href="amp/index.html">
    
    <meta property="og:site_name" content="愚人拙记">
    <meta property="og:type" content="article">
    <meta property="og:title" content="PRML.Ch10读书笔记：变分推理">
    <meta property="og:description" content="0 疑问 这类概率推理问题在没有VB方法的时候都是怎么求解的？ VB的直接好处是什么？ 什么是平均场估计？ 这里的估计方法和概率图中的BP的具体关系？ VB中每一步的模型都是设定好的吗？例如LDA中使用Dirichlet作为后验概率？ LDA中的VB是如何推导的？ 1 引子 本文是PRML第10章部分内容的摘录和总结。在很多概率问题中，如果使用精确求解，那么问题规模与随机变量的个数是指数上升的。以主题模型LDA为例，每个词的生成对应一个随机变量，使用确定性的方法会导致问题规模为$K^{NM}$。现有的估计方法包括变分推导、随机模拟/采样、MCMC方法。其中变分推理是一个实现框架，具体而言有Loopy belief propagation方法和Mean field approximation方法。为了简单，以下VB即是说变分推理。 用最简单的话来讲，VB是说数据集中很多特性经过简单统计能反映原始参数的多少，每次迭代我们首先在E步对这些特性进行统计（实际上是求解充分统计量），之后，在M步看在这些统计结果的限制内，参数最可能是多少。这些特性可能是一些计数结果等，例如在LDA模型中，可能是属于不同主题的词的个数等等。有意思的是，在这个角度上VB方法与采样方法有着很大的相似点，唯一不同的是，VB方法每次迭代有明确的前进方向，而采样是靠数量取胜，从这里也能看出VB和采样的优势分别是速度和精度。 2 核心思想">
    <meta property="og:url" content="http://localhost:2368/prml-ch10du-shu-bi-ji-bian-fen-tui-li/">
    <meta property="article:published_time" content="2016-08-15T11:38:00.000Z">
    <meta property="article:modified_time" content="2017-09-03T12:23:18.000Z">
    <meta property="article:tag" content="机器学习">
    <meta property="article:tag" content="读书笔记">
    
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="PRML.Ch10读书笔记：变分推理">
    <meta name="twitter:description" content="0 疑问 这类概率推理问题在没有VB方法的时候都是怎么求解的？ VB的直接好处是什么？ 什么是平均场估计？ 这里的估计方法和概率图中的BP的具体关系？ VB中每一步的模型都是设定好的吗？例如LDA中使用Dirichlet作为后验概率？ LDA中的VB是如何推导的？ 1 引子 本文是PRML第10章部分内容的摘录和总结。在很多概率问题中，如果使用精确求解，那么问题规模与随机变量的个数是指数上升的。以主题模型LDA为例，每个词的生成对应一个随机变量，使用确定性的方法会导致问题规模为$K^{NM}$。现有的估计方法包括变分推导、随机模拟/采样、MCMC方法。其中变分推理是一个实现框架，具体而言有Loopy belief propagation方法和Mean field approximation方法。为了简单，以下VB即是说变分推理。 用最简单的话来讲，VB是说数据集中很多特性经过简单统计能反映原始参数的多少，每次迭代我们首先在E步对这些特性进行统计（实际上是求解充分统计量），之后，在M步看在这些统计结果的限制内，参数最可能是多少。这些特性可能是一些计数结果等，例如在LDA模型中，可能是属于不同主题的词的个数等等。有意思的是，在这个角度上VB方法与采样方法有着很大的相似点，唯一不同的是，VB方法每次迭代有明确的前进方向，而采样是靠数量取胜，从这里也能看出VB和采样的优势分别是速度和精度。 2 核心思想">
    <meta name="twitter:url" content="http://localhost:2368/prml-ch10du-shu-bi-ji-bian-fen-tui-li/">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="zhaodaolimeng">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="机器学习, 读书笔记">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "愚人拙记",
        "logo": {
            "@type": "ImageObject",
            "url": "http://localhost:2368/favicon.ico",
            "width": 60,
            "height": 60
        }
    },
    "author": {
        "@type": "Person",
        "name": "zhaodaolimeng",
        "image": {
            "@type": "ImageObject",
            "url": "//www.gravatar.com/avatar/731e3411d9e4ccc30af1dcd7c4c5af9f?s=250&d=mm&r=x",
            "width": 250,
            "height": 250
        },
        "url": "http://localhost:2368/author/zhaodaolimeng/",
        "sameAs": []
    },
    "headline": "PRML.Ch10读书笔记：变分推理",
    "url": "http://localhost:2368/prml-ch10du-shu-bi-ji-bian-fen-tui-li/",
    "datePublished": "2016-08-15T11:38:00.000Z",
    "dateModified": "2017-09-03T12:23:18.000Z",
    "keywords": "机器学习, 读书笔记",
    "description": "0 疑问 这类概率推理问题在没有VB方法的时候都是怎么求解的？ VB的直接好处是什么？ 什么是平均场估计？ 这里的估计方法和概率图中的BP的具体关系？ VB中每一步的模型都是设定好的吗？例如LDA中使用Dirichlet作为后验概率？ LDA中的VB是如何推导的？ 1 引子 本文是PRML第10章部分内容的摘录和总结。在很多概率问题中，如果使用精确求解，那么问题规模与随机变量的个数是指数上升的。以主题模型LDA为例，每个词的生成对应一个随机变量，使用确定性的方法会导致问题规模为$K^{NM}$。现有的估计方法包括变分推导、随机模拟/采样、MCMC方法。其中变分推理是一个实现框架，具体而言有Loopy belief propagation方法和Mean field approximation方法。为了简单，以下VB即是说变分推理。 用最简单的话来讲，VB是说数据集中很多特性经过简单统计能反映原始参数的多少，每次迭代我们首先在E步对这些特性进行统计（实际上是求解充分统计量），之后，在M步看在这些统计结果的限制内，参数最可能是多少。这些特性可能是一些计数结果等，例如在LDA模型中，可能是属于不同主题的词的个数等等。有意思的是，在这个角度上VB方法与采样方法有着很大的相似点，唯一不同的是，VB方法每次迭代有明确的前进方向，而采样是靠数量取胜，从这里也能看出VB和采样的优势分别是速度和精度。 2 核心思想",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:2368/"
    }
}
    </script>

    <script type="text/javascript" src="../public/ghost-sdk.js?v=0be4e97106"></script>
<script type="text/javascript">
ghost.init({
	clientId: "ghost-frontend",
	clientSecret: "934a083a6285"
});
</script>
    <meta name="generator" content="Ghost 1.7">
    <link rel="alternate" type="application/rss+xml" title="愚人拙记" href="../rss/index.html">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [["$", "$"], ["\\(", "\\)"]],
        processEscapes: true
    }});
</script>

</head>
<body class="post-template tag-ji-qi-xue-xi tag-du-shu-bi-ji">

    <div class="site-wrapper">

        

<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
                <a class="site-nav-logo" href="../">愚人拙记</a>
            <ul class="nav">
    <li class="nav-home" role="presentation"><a href="../">Home</a></li>
</ul>
    </div>
    <div class="site-nav-right">
        <div class="social-links">
        </div>
            <a class="rss-button" href="http://cloud.feedly.com/#subscription/feed/http://localhost:2368/rss/" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24"><circle cx="6.18" cy="17.82" r="2.18"></circle><path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"></path></svg>
</a>
    </div>
</nav>
    </div>
</header>


<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full post tag-ji-qi-xue-xi tag-du-shu-bi-ji no-image">

            <header class="post-full-header">
                <section class="post-full-meta">
                    <time class="post-full-meta-date" datetime="2016-08-15">15 August 2016</time>
                        <span class="date-divider">/</span> <a href="../tag/ji-qi-xue-xi/">机器学习</a>
                </section>
                <h1 class="post-full-title">PRML.Ch10读书笔记：变分推理</h1>
            </header>


            <section class="post-full-content">
                <div class="kg-card-markdown"><h1 id="0">0 疑问</h1>
<p>这类概率推理问题在没有VB方法的时候都是怎么求解的？<br>
VB的直接好处是什么？<br>
什么是平均场估计？<br>
这里的估计方法和概率图中的BP的具体关系？<br>
VB中每一步的模型都是设定好的吗？例如LDA中使用Dirichlet作为后验概率？<br>
LDA中的VB是如何推导的？</p>
<h1 id="1">1 引子</h1>
<p>本文是PRML第10章部分内容的摘录和总结。在很多概率问题中，如果使用精确求解，那么问题规模与随机变量的个数是指数上升的。以主题模型LDA为例，每个词的生成对应一个随机变量，使用确定性的方法会导致问题规模为$K^{NM}$。现有的估计方法包括变分推导、随机模拟/采样、MCMC方法。其中变分推理是一个实现框架，具体而言有Loopy belief propagation方法和Mean field approximation方法。为了简单，以下VB即是说变分推理。</p>
<p>用最简单的话来讲，VB是说数据集中很多特性经过简单统计能反映原始参数的多少，每次迭代我们首先在E步对这些特性进行统计（实际上是求解充分统计量），之后，在M步看在这些统计结果的限制内，参数最可能是多少。这些特性可能是一些计数结果等，例如在LDA模型中，可能是属于不同主题的词的个数等等。有意思的是，在这个角度上VB方法与采样方法有着很大的相似点，唯一不同的是，VB方法每次迭代有明确的前进方向，而采样是靠数量取胜，从这里也能看出VB和采样的优势分别是速度和精度。</p>
<h1 id="2">2 核心思想</h1>
<p>变分推理的最终目的是要找到一个形式简单的分布Q，使得其能较好地估计形式复杂的真实分布P的取值。当我们指定一个Q之后，可以列出Q与P的关系：</p>
<p>$$\ln{p(\bf{X})}=\mathcal{L}(q)+KL(q||p)\qquad(1)$$</p>
<p>其中，</p>
<p>$$\mathcal{L}(q)=\int{q(Z)\ln{\frac{p(X,Z)}{q(Z)}}dZ}$$</p>
<p>$$KL(q||p)=-\int{q(Z)\ln{\frac{p{(Z|X)}}{q(Z)}}dZ}$$</p>
<p>这里我们使用KL散度描述P与Q的近似程度。KL散度是似然比的对数期望，它也是确定q之后p的混乱程度。另外，由于因为q与p不同分布时   $KL(p\vert\vert q) \neq KL(q\vert\vert p)$ ，所以我们实际上面临 $KL(q\vert\vert p)$ 和 $KL(p\vert\vert q)$ 两个选择，实际情况是前者更为合理。如果我们能获得$Z$的解析形式的关系，那么参照EM方法中迭代求解隐变量的思路，即可求解隐变量的随机分布。VB与EM的最大区别在于VB中不再出现模型参数，取而代之的是随机变量。</p>
<h2 id="21klqvertvertp">2.1 为何使用 $KL(q\vert\vert p)$</h2>
<p><img src="../content/images/2017/09/em.png" alt="em"></p>
<p>$KL(q\vert\vert p)$ 更倾向于使 $q$ 去精确拟合 $p$ 概率密度为0时的位置，这就导致对于分离的概率密度函数，$q$ 会产生一种聚集效果，即像后两个图一样拟合其中一个分离的分布，而不是像(a)一样试图拟合非0位置，这种行为叫做model-seeking。</p>
<h2 id="22q">2.2 分布Q的合理形式</h2>
<p>这种合理形式叫做可分解分布，满足：</p>
<p>$$q(Z)=\prod_{i=1}^{M} q_i(Z_i)$$</p>
<p>使用这种假设的好处是可将原始分布分解为多个较低维度的成分，可简化计算，这种方法在统计物理中被称为平均场方法。回顾公式(1)，我们的VB的最终目标是求一个Q，使得Q与P的KL距离最小，这等价于 $\mathcal{L}(q)$ 的最大化。事实上，由(1)式可直接获得如下关系：</p>
<p>$$\mathcal{L}(q)=\int{\ln{p(X,Z)}-\sum_{i}{\ln{q_i}}}\prod_{i}{q_i(x_i)}dZ$$<br>
$$=\int q_{j}\ln{\tilde{p}(X,Z_j)dZ_j}-\int{q_j\ln q_j}dZ_j+\text{const}$$</p>
<p>以上公式是为了获得 $q_j$ 和其他 $q$ 的关系，以析解得目标(1)的最优解。推导过程中注意积分变量和提出被积变量中的常量。回顾公式(1)，我们令KL散度直接为0使 $\mathcal{L}(q)=\ln(p)$，可得以下公式：</p>
<p>$$<br>
\ln{q^\star_{j_{(\bf{Z_j})}}}=\mathbb{E}_{i\neq{j}}\ln{p(\bf{X},\bf{Z})}]+\text{const}\qquad(2)<br>
$$</p>
<p>结论就是：为了估计随机变量$q_j$的分布，需要对其他所有随机变量的求期望，这样就极小化了KL散度，即使得Q与P更为接近。</p>
<h1 id="3">3 实例</h1>
<p>VB方法具有一个统一的推导求解框架，但对于不同的模型往往会有不同的insight，PRML中也从不同的方向进行了求解。</p>
<h2 id="31">3.1 二元高斯模型</h2>
<p>（待补充）</p>
<h2 id="32">3.2 混合高斯模型</h2>
<p>首先将GMM模型进行贝叶斯化，GMM的生成模型如下：</p>
<p>$$<br>
\alpha_0 \rightarrow \pi \rightarrow Z \rightarrow X \leftarrow \mu,\Lambda<br>
$$</p>
<p>其中，X为观测变量，大小为1*N；Z为每个观测变量在不同类别中的归属，使用01表示，大小为是K*N；$\pi$ 为不同类别的权重，大小为1*K； $\alpha_0$ 为决定 $\pi$ 形态的超参数，大小为1*K； $\mu$和 $\Lambda$ 本身为每个正态分量的均值和方差参数。其中，变量间的关系如下：</p>
<p>$$p(X|Z,\mu,\Lambda)=\prod_{n=1}^N\prod_{k=1}^K\mathcal{N}(x_n|\mu_k,\Lambda_k^{-1})^{z_{nk}}$$</p>
<p>$$p(Z|\pi) = \prod_{n=1}^{N}\prod_{k=1}^K\pi_{k}^{z_{nk}}$$</p>
<p>$$p(\pi)=\text{Dir}(\pi|\alpha_0)=C(\alpha_0)\prod_{k=1}^K{\pi_k^{\alpha_0-1}}$$</p>
<p>$$p(\mu,\Lambda)=\prod_{k=1}^{K}{\mathcal{N}(\mu_k|m_0,(\beta_0\Lambda_kk)^{-1})\mathcal{W}(\Lambda_k|W_0,v_0)}$$</p>
<p>可以看出 $p(Z\vert\pi)$ 是以 $\pi$ 为参数的多项分布。 $p(\pi\vert\alpha_0)$ 可使用Dirichlet分布进行描述，正态分布的参数可使用Gaussian-Wishart分布描述，因为他们分别是多项分布和高斯分布的先验共轭。</p>
<p>最终目标实际上是估计以上关系构成的联合分布  $p(X,Z,\pi,\mu,\Lambda)$ 。我们使用2.2节中提到的可分解分布  $q(Z)q(\pi,\mu,\Lambda)$ 对 $p$ 进行估计。一种直观的方式是分解整个联合概率分布，以构造分布Q，如下：</p>
<p>$$<br>
\ln q^\star(Z)=\mathbb{E}_{\pi,\mu,\Lambda}[\ln{p(X,Z,\pi,\mu,\Lambda)}] + \text{const}\quad(3)<br>
$$</p>
<p>将 $p$ 的生成模型代入其中，直接将(3)展开，可得到一种分布分解的情况。</p>
<p>$$\ln q^\star(Z)=\sum_{n=1}^N\sum_{k=1}^K z_{nk} \ln{r_{nk}}$$</p>
<p>这里没有将 $r$ 进行展开，其意义为Q函数中一个规范化后的 $q$ 。观察到 $r$ 的形式和我们想要的Q函数的形式一致，所以我们直接令 $\mathbb{E}[z_{nk}]= r_{nk}$ 可得到VB的参数推导公式。但遗憾的是这种方法得到的结果与MLE是一致的，而事实证明MLE方法在对应GMM模型中有较为明显的缺陷，即从整个联合概率密度进行估计Q函数并不是一个好的方式。</p>
<p>一个更合适的方式是尝试将 $\ln{q^\star}(\pi,\mu,\Lambda)$ 进行分布的分解。E步使用期望估计Q函数的形式，待估计函数的展开形式仍然如(3)所示，该步骤可产生关键的模型参数 $r_{nk}$ ：<br>
$$<br>
\mathbb{E}_{\mu_k,\Lambda_k}[(x_n-\mu_k)^T\Lambda_k(x_n-\mu_k)]=D\beta_k^{-1} + v_k(x_n-m_k)^TW_k(x_n-m_k)<br>
$$</p>
<p>$$<br>
\ln{\tilde\Lambda_k}=\mathbb{E}[\ln|\Lambda_k|]=\sum_{i=1}^D\psi(\frac{v_k+1-i}{2})+D\ln 2 +\ln|W_k|<br>
$$</p>
<p>$$<br>
\ln\tilde\pi_k=\mathbb{E}[\ln\pi_k]=\psi(\alpha_k)-\psi(\sum_k\alpha_k)<br>
$$</p>
<p>以上公式带入(3)中可获得：</p>
<p>$$<br>
r_{nk}\propto \tilde\pi_k\tilde\Lambda_k^{1/2}\exp{-\frac{D}{2\beta_k}-\frac{v_k}{2}(x_n-m_k)^T W_k(x_n-m_k)}<br>
$$</p>
<p>M步更新原始模型， $r_{nk}$ 为计算原始模型的参数：</p>
<p>$$q^\star(\pi)=\text{Dir}(\pi|\alpha)$$</p>
<p>$$q^\star(\mu_k,\Lambda_k)=\mathcal{N}(\mu_k|m_k,(\beta_k\Lambda_k)^{-1})\mathcal{W}(\Lambda_k|W_k,v_k)$$</p>
<p>其中，</p>
<p>$$\beta_k=\beta_0+N_k$$<br>
$$m_k=\frac{1}{\beta_k}(\beta_0 m_0 + N_k \bar{x}_k)$$<br>
$$W_k^{-1}=W_0^{-1}+N_k S_k + \frac{\beta_0 N_k}{\beta_0+N_k}(\bar x_k - m_0)(\bar x_k - m_0)^T$$<br>
$$v_k = v_0 + N_k$$</p>
<p>以上分布分解方法最终得到的Q函数计算结果与EM方法是一致的。</p>
<h2 id="33">3.3 贝叶斯线性回归</h2>
<p>（待补充）</p>
<h1 id="4">4 参考文献</h1>
<p><a href="http://www.blog.huajh7.com/variational-bayes/">中文博客huajh7, Variational Bayes</a><br>
<a href="http://www.52nlp.cn/prml%E8%AF%BB%E4%B9%A6%E4%BC%9A%E7%AC%AC%E5%8D%81%E7%AB%A0-approximate-inference">PRML读书会第十章</a><br>
<a href="http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf">C. Bishop, PRML Chapter10</a><br>
<a href="https://www.cs.cmu.edu/~epxing/Class/10708-14/lecture.html">E. Xing, Probabilistic Graphical Models</a><br>
<a href="http://www.cse.buffalo.edu/faculty/mbeal/thesis/">B. Matthew, PhD thesis</a></p>
</div>
            </section>


            <footer class="post-full-footer">

                <section class="author-card">
                        <img class="author-profile-image" src="http://www.gravatar.com/avatar/731e3411d9e4ccc30af1dcd7c4c5af9f?s=250&amp;d=mm&amp;r=x" alt="zhaodaolimeng">
                    <section class="author-card-content">
                        <h4 class="author-card-name"><a href="../author/zhaodaolimeng/">zhaodaolimeng</a></h4>
                            <p>Read <a href="../author/zhaodaolimeng/">more posts</a> by this author.</p>
                    </section>
                </section>
                <div class="post-full-footer-right">
                    <a class="author-card-button" href="../author/zhaodaolimeng/">Read More</a>
                </div>

            </footer>


        </article>

        <!-- UY BEGIN -->
        <div id="uyan_frame"></div>
        <script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=2143114"></script>
        <!-- UY END -->

    </div>
</main>

<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">

                <article class="read-next-card" style="background-image: url(../content/images/2017/09/tahiti-vacation-packages@2x.jpg)">
                    <header class="read-next-card-header">
                        <small class="read-next-card-header-sitetitle">— 愚人拙记 —</small>
                        <h3 class="read-next-card-header-title"><a href="../tag/ji-qi-xue-xi/">机器学习</a></h3>
                    </header>
                    <div class="read-next-divider"><svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24"><path d="M13 14.5s2 3 5 3 5.5-2.463 5.5-5.5S21 6.5 18 6.5c-5 0-7 11-12 11C2.962 17.5.5 15.037.5 12S3 6.5 6 6.5s4.5 3.5 4.5 3.5"></path></svg>
</div>
                    <div class="read-next-card-content">
                        <ul>
                            <li><a href="../snorkelxue-xi-bi-ji/">Snorkel学习笔记</a></li>
                            <li><a href="../deepdivexue-xi-bi-ji/">Deepdive学习笔记</a></li>
                            <li><a href="../ldamo-xing-ru-men/">LDA模型入门</a></li>
                        </ul>
                    </div>
                    <footer class="read-next-card-footer">
                        <a href="../tag/ji-qi-xue-xi/">See all 4 posts →</a>
                    </footer>
                </article>

                <article class="post-card post tag-ji-qi-xue-xi no-image">
    <div class="post-card-content">
        <a class="post-card-content-link" href="../ldamo-xing-ru-men/">
            <header class="post-card-header">
                    <span class="post-card-tags">机器学习</span>
                <h2 class="post-card-title">LDA模型入门</h2>
            </header>
            <section class="post-card-excerpt">
                <p>1 引子 本文是对Blei等人LDA原始论文的总结。给定大量的文档，如何在无标注的情况下确定每个文档的主题词？LDA(Latent Dirichlet Allocation)是这类主题确定问题的一个成熟的解决方案。LDA最初面向文本挖掘领域，但随后在图像分类、行为识别等领域也得到了应用。LDA是一种典型的非监督模型，模型仅需要输入文档集合的词袋模型，模型可输出每个文档对应的主题，每个主题使用关键词的分布来表示。 2 模型定义 LDA的PGM形式如上，我们认为主题数目有K个，文档有M个， 每个文档中有N个词。其中，$\alpha$ 是Dirichlet分布的参数，大小为1xK，用于控制生成主题的聚集程度； $\theta$ 表示一个文档中主题的分布大小为1xK； $z$ 为一个为每个词安排主题的01随机变量，大小为1xK，</p>
            </section>
        </a>
        <footer class="post-card-meta">
                <img class="author-profile-image" src="http://www.gravatar.com/avatar/731e3411d9e4ccc30af1dcd7c4c5af9f?s=250&amp;d=mm&amp;r=x" alt="zhaodaolimeng">
            <span class="post-card-author"><a href="../author/zhaodaolimeng/">zhaodaolimeng</a></span>
        </footer>
    </div>
</article>

                <article class="post-card post tag-ji-qi-xue-xi tag-du-shu-bi-ji no-image">
    <div class="post-card-content">
        <a class="post-card-content-link" href="../prml-ch9du-shu-bi-ji-emfang-fa/">
            <header class="post-card-header">
                    <span class="post-card-tags">机器学习</span>
                <h2 class="post-card-title">PRML.Ch9读书笔记：EM方法</h2>
            </header>
            <section class="post-card-excerpt">
                <p>1 引子 本文涉及EM的使用场景区别、理论推导。以实践的角度，EM方法是一个迭代优化以求解隐变量的方法。本文内容是PRML第九章的缩减。 2 EM用于GMM模型 2.1 极大似然尝试求解GMM参数 以GMM为例，这里先试图使用最大似然估计的方式求解参数： $ p(x|\pi,\mu,\Sigma)=\sum_{k=1}^{K}\pi_{k}\mathcal{N}(x|\mu_{k},\Sigma_{k}) $ 最终目标是求解两部分内容：</p>
            </section>
        </a>
        <footer class="post-card-meta">
                <img class="author-profile-image" src="http://www.gravatar.com/avatar/731e3411d9e4ccc30af1dcd7c4c5af9f?s=250&amp;d=mm&amp;r=x" alt="zhaodaolimeng">
            <span class="post-card-author"><a href="../author/zhaodaolimeng/">zhaodaolimeng</a></span>
        </footer>
    </div>
</article>

        </div>
    </div>
</aside>

<div class="floating-header">
    <div class="floating-header-logo">
        <a href="../">
            <span>愚人拙记</span>
        </a>
    </div>
    <span class="floating-header-divider">—</span>
    <div class="floating-header-title">PRML.Ch10读书笔记：变分推理</div>
    <div class="floating-header-share">
        <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
    <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"></path>
</svg>
</div>
        <a class="floating-header-share-tw" href="https://twitter.com/share?text=PRML.Ch10%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%8F%98%E5%88%86%E6%8E%A8%E7%90%86&amp;url=http://localhost:2368/prml-ch10du-shu-bi-ji-bian-fen-tui-li/" onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"></path></svg>
        </a>
        <a class="floating-header-share-fb" href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:2368/prml-ch10du-shu-bi-ji-bian-fen-tui-li/" onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"></path></svg>
        </a>
    </div>
    <progress class="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>
</div>




        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="../">愚人拙记</a> © 2017</section>
                <nav class="site-footer-nav">
                    <a href="../">Latest Posts</a>
                    
                    
                    <a href="https://ghost.org" target="_blank">Ghost</a>
                </nav>
            </div>
        </footer>

    </div>


    <script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="../assets/js/jquery.fitvids.js?v=0be4e97106"></script>


    <script>

// NOTE: Scroll performance is poor in Safari
// - this appears to be due to the events firing much more slowly in Safari.
//   Dropping the scroll event and using only a raf loop results in smoother
//   scrolling but continuous processing even when not scrolling
$(document).ready(function () {
    // Start fitVids
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    // End fitVids

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        // show/hide floating header
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>


    

</body>
