
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>PRML.Ch9读书笔记：EM方法</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="stylesheet" type="text/css" href="../assets/built/screen.css?v=f222684241">

    <link rel="shortcut icon" href="../favicon.ico" type="image/x-icon">
    <link rel="canonical" href="index.html">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <link rel="amphtml" href="amp/index.html">
    
    <meta property="og:site_name" content="愚人拙记">
    <meta property="og:type" content="article">
    <meta property="og:title" content="PRML.Ch9读书笔记：EM方法">
    <meta property="og:description" content="1 引子 本文涉及EM的使用场景区别、理论推导。以实践的角度，EM方法是一个迭代优化以求解隐变量的方法。本文内容是PRML第九章的缩减。 2 EM用于GMM模型 2.1 极大似然尝试求解GMM参数 以GMM为例，这里先试图使用最大似然估计的方式求解参数： $ p(x|\pi,\mu,\Sigma)=\sum_{k=1}^{K}\pi_{k}\mathcal{N}(x|\mu_{k},\Sigma_{k}) $ 最终目标是求解两部分内容：未观测变量和模型参数。个人理解对于GMM，其未观测变量可明确地指定为 $\pi_{k}$，而其模型参数确定为 $\mu_k$和 $\Sigma_k$。这里优化目标是当前的估计导致的损失，或者说对数似然函数： $\ln{p(X|\pi,">
    <meta property="og:url" content="http://localhost:2368/prml-ch9du-shu-bi-ji-emfang-fa/">
    <meta property="article:published_time" content="2016-08-12T12:52:00.000Z">
    <meta property="article:modified_time" content="2017-09-03T11:31:08.000Z">
    <meta property="article:tag" content="机器学习">
    <meta property="article:tag" content="读书笔记">
    
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="PRML.Ch9读书笔记：EM方法">
    <meta name="twitter:description" content="1 引子 本文涉及EM的使用场景区别、理论推导。以实践的角度，EM方法是一个迭代优化以求解隐变量的方法。本文内容是PRML第九章的缩减。 2 EM用于GMM模型 2.1 极大似然尝试求解GMM参数 以GMM为例，这里先试图使用最大似然估计的方式求解参数： $ p(x|\pi,\mu,\Sigma)=\sum_{k=1}^{K}\pi_{k}\mathcal{N}(x|\mu_{k},\Sigma_{k}) $ 最终目标是求解两部分内容：未观测变量和模型参数。个人理解对于GMM，其未观测变量可明确地指定为 $\pi_{k}$，而其模型参数确定为 $\mu_k$和 $\Sigma_k$。这里优化目标是当前的估计导致的损失，或者说对数似然函数： $\ln{p(X|\pi,">
    <meta name="twitter:url" content="http://localhost:2368/prml-ch9du-shu-bi-ji-emfang-fa/">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="zhaodaolimeng">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="机器学习, 读书笔记">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "愚人拙记",
        "logo": {
            "@type": "ImageObject",
            "url": "http://localhost:2368/favicon.ico",
            "width": 60,
            "height": 60
        }
    },
    "author": {
        "@type": "Person",
        "name": "zhaodaolimeng",
        "image": "//www.gravatar.com/avatar/731e3411d9e4ccc30af1dcd7c4c5af9f?s=250&d=mm&r=x",
        "url": "http://localhost:2368/author/zhaodaolimeng/",
        "sameAs": []
    },
    "headline": "PRML.Ch9读书笔记：EM方法",
    "url": "http://localhost:2368/prml-ch9du-shu-bi-ji-emfang-fa/",
    "datePublished": "2016-08-12T12:52:00.000Z",
    "dateModified": "2017-09-03T11:31:08.000Z",
    "keywords": "机器学习, 读书笔记",
    "description": "1 引子 本文涉及EM的使用场景区别、理论推导。以实践的角度，EM方法是一个迭代优化以求解隐变量的方法。本文内容是PRML第九章的缩减。 2 EM用于GMM模型 2.1 极大似然尝试求解GMM参数 以GMM为例，这里先试图使用最大似然估计的方式求解参数： $ p(x|\\pi,\\mu,\\Sigma)&#x3D;\\sum_{k&#x3D;1}^{K}\\pi_{k}\\mathcal{N}(x|\\mu_{k},\\Sigma_{k}) $ 最终目标是求解两部分内容：未观测变量和模型参数。个人理解对于GMM，其未观测变量可明确地指定为 $\\pi_{k}$，而其模型参数确定为 $\\mu_k$和 $\\Sigma_k$。这里优化目标是当前的估计导致的损失，或者说对数似然函数： $\\ln{p(X|\\pi,",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:2368/"
    }
}
    </script>

    <script type="text/javascript" src="../public/ghost-sdk.js?v=f222684241"></script>
<script type="text/javascript">
ghost.init({
	clientId: "ghost-frontend",
	clientSecret: "934a083a6285"
});
</script>
    <meta name="generator" content="Ghost 1.7">
    <link rel="alternate" type="application/rss+xml" title="愚人拙记" href="../rss/index.html">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [["$", "$"], ["\\(", "\\)"]],
        processEscapes: true
    }});
</script>

</head>
<body class="post-template tag-ji-qi-xue-xi tag-du-shu-bi-ji">

    <div class="site-wrapper">

        

<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
                <a class="site-nav-logo" href="../">愚人拙记</a>
            <ul class="nav">
    <li class="nav-home" role="presentation"><a href="../">Home</a></li>
</ul>
    </div>
    <div class="site-nav-right">
        <div class="social-links">
        </div>
            <a class="rss-button" href="http://cloud.feedly.com/#subscription/feed/http://localhost:2368/rss/" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24"><circle cx="6.18" cy="17.82" r="2.18"></circle><path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"></path></svg>
</a>
    </div>
</nav>
    </div>
</header>


<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full post tag-ji-qi-xue-xi tag-du-shu-bi-ji no-image">

            <header class="post-full-header">
                <section class="post-full-meta">
                    <time class="post-full-meta-date" datetime="2016-08-12">12 August 2016</time>
                        <span class="date-divider">/</span> <a href="../tag/ji-qi-xue-xi/">机器学习</a>
                </section>
                <h1 class="post-full-title">PRML.Ch9读书笔记：EM方法</h1>
            </header>


            <section class="post-full-content">
                <div class="kg-card-markdown"><h1 id="1">1 引子</h1>
<p>本文涉及EM的使用场景区别、理论推导。以实践的角度，EM方法是一个迭代优化以求解隐变量的方法。本文内容是PRML第九章的缩减。</p>
<h1 id="2emgmm">2 EM用于GMM模型</h1>
<h2 id="21gmm">2.1 极大似然尝试求解GMM参数</h2>
<p>以GMM为例，这里先试图使用最大似然估计的方式求解参数：<br>
$$<br>
p(x|\pi,\mu,\Sigma)=\sum_{k=1}^{K}\pi_{k}\mathcal{N}(x|\mu_{k},\Sigma_{k})<br>
$$<br>
最终目标是求解两部分内容：未观测变量和模型参数。个人理解对于GMM，其未观测变量可明确地指定为 $\pi_{k}$，而其模型参数确定为 $\mu_k$和 $\Sigma_k$。这里优化目标是当前的估计导致的损失，或者说对数似然函数：</p>
<p>$$\ln{p(X|\pi,\mu,\Sigma)}=<br>
\sum_{n=1}^{N}{\ln\sum_{k=1}^K{\pi_k\mathcal{N}(x_n|\mu_k,\Sigma_k)}}$$</p>
<p>以上问题由于隐变量的存在，同时由于参数在正态分布的积分中，一般来说是难解的。具体地，对 $\ln{p(X|\pi,\mu,\Sigma)}$ 求导，并令导数为0可以看出隐变量和参数之间的关系：</p>
<p>$$<br>
\frac{\partial{\ln{p(X|\pi,\mu,\Sigma)}}}{\partial{\mu_k}}<br>
=-\sum_{n=1}^{N} \gamma(z_{nk})\Sigma_k(x_n-\mu_k)=0<br>
$$</p>
<p>$$<br>
\frac{\partial{\ln{p(X|\pi,\mu,\Sigma)}}}{\partial{\Sigma_k}}<br>
=\sum_{n=1}^N \gamma(z_{nk}) {<br>
-\frac{N}{2}\Sigma^{-1}+\frac{N}{2}\Sigma^{-1}\sum_{d=1}^{D}(x_i-\mu)^T \Sigma^{-1}_k (x_i-\mu)\Sigma^{-1}<br>
} =0<br>
$$</p>
<p>其中，$\gamma(z_{nk})$ 的物理意义是第n个观测在第k簇的概率，形式为：</p>
<p>$$<br>
\gamma(z_{nk})=\frac{\pi_k\mathcal{N}(x_n|\mu_k,\Sigma_k)}{\sum_j{\pi_j\mathcal{N}(x_n|\mu_j,\Sigma_j)}}<br>
$$</p>
<p>具体的结果可参考PRML。使用以上两个等式，原则上可计算参数和未观测量的值，这里是为了展现：由于对数中本身有加和的形式，这种方式难以获得解析解。需要有一个更便捷的框架解决以上参数求解问题。</p>
<h2 id="22emgmm">2.2 EM方法估计GMM参数</h2>
<p>EM方法正是这样一个框架：套用以上的结果，使用迭代的方法通过不断修正找到一个函数$q(x)$ ，使得$q(x)$ 与$p(x)$ 接近，那么即可使用$q(x)$ 对最终结果进行近似。具体的步骤如下：</p>
<p>(1) 初始化参数 $\mu_k$、$\Sigma_k$和未观测值 $\pi_k$。一个可行的方式是，由于K-means迭代次数较快，可使用K-means对数据进行预处理，然后选择K-means的中心点作为 $\mu_k$的初值。<br>
(2) E步，固定模型参数，优化未观测变量：</p>
<p>$$<br>
\gamma(z_{nk})=\frac{\pi_k\mathcal{N}(x_n|\mu_k,\Sigma_k)}{\sum_j{\pi_j\mathcal{N}(x_n|\mu_j,\Sigma_j)}}<br>
$$</p>
<p>(3) M步，M步将固定未观测变量，优化模型参数：</p>
<p>$$<br>
\mu_k^{new}=\frac{1}{N_k}\sum_{n=1}^{N}\gamma(z_{nk})\bf{x}_n<br>
$$</p>
<p>$$<br>
\Sigma_k^{new}=\frac{1}{N_k}\sum_{n=1}^{N}\gamma(z_{nk})(\bf{x}_n-\mu_k^{new})(\bf{x}_n-\mu_k^{new})^T<br>
$$</p>
<p>(4) 计算likehood，如果结果收敛则停止。</p>
<h1 id="3em">3 EM方法正确性</h1>
<p>（待续）</p>
</div>
            </section>


            <footer class="post-full-footer">

                <section class="author-card">
                        <img class="author-profile-image" src="http://www.gravatar.com/avatar/731e3411d9e4ccc30af1dcd7c4c5af9f?s=250&amp;d=mm&amp;r=x" alt="zhaodaolimeng">
                    <section class="author-card-content">
                        <h4 class="author-card-name"><a href="../author/zhaodaolimeng/">zhaodaolimeng</a></h4>
                            <p>Read <a href="../author/zhaodaolimeng/">more posts</a> by this author.</p>
                    </section>
                </section>
                <div class="post-full-footer-right">
                    <a class="author-card-button" href="../author/zhaodaolimeng/">Read More</a>
                </div>

            </footer>


        </article>

        <!-- UY BEGIN -->
        <div id="uyan_frame"></div>
        <script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=2143114"></script>
        <!-- UY END -->

    </div>
</main>

<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">

                <article class="read-next-card" style="background-image: url(../content/images/2017/09/tahiti-vacation-packages@2x.jpg)">
                    <header class="read-next-card-header">
                        <small class="read-next-card-header-sitetitle">— 愚人拙记 —</small>
                        <h3 class="read-next-card-header-title"><a href="../tag/ji-qi-xue-xi/">机器学习</a></h3>
                    </header>
                    <div class="read-next-divider"><svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24"><path d="M13 14.5s2 3 5 3 5.5-2.463 5.5-5.5S21 6.5 18 6.5c-5 0-7 11-12 11C2.962 17.5.5 15.037.5 12S3 6.5 6 6.5s4.5 3.5 4.5 3.5"></path></svg>
</div>
                    <div class="read-next-card-content">
                        <ul>
                            <li><a href="../snorkelxue-xi-bi-ji/">Snorkel学习笔记</a></li>
                            <li><a href="../deepdivexue-xi-bi-ji/">Deepdive学习笔记</a></li>
                            <li><a href="../ldamo-xing-ru-men/">LDA模型入门</a></li>
                        </ul>
                    </div>
                    <footer class="read-next-card-footer">
                        <a href="../tag/ji-qi-xue-xi/">See all 4 posts →</a>
                    </footer>
                </article>

                <article class="post-card post tag-ji-qi-xue-xi tag-du-shu-bi-ji no-image">
    <div class="post-card-content">
        <a class="post-card-content-link" href="../prml-ch10du-shu-bi-ji-bian-fen-tui-li/">
            <header class="post-card-header">
                    <span class="post-card-tags">机器学习</span>
                <h2 class="post-card-title">PRML.Ch10读书笔记：变分推理</h2>
            </header>
            <section class="post-card-excerpt">
                <p>0 疑问 这类概率推理问题在没有VB方法的时候都是怎么求解的？ VB的直接好处是什么？ 什么是平均场估计？ 这里的估计方法和概率图中的BP的具体关系？ VB中每一步的模型都是设定好的吗？例如LDA中使用Dirichlet作为后验概率？ LDA中的VB是如何推导的？ 1 引子 本文是PRML第10章部分内容的摘录和总结。在很多概率问题中，如果使用精确求解，那么问题规模与随机变量的个数是指数上升的。以主题模型LDA为例，每个词的生成对应一个随机变量，使用确定性的方法会导致问题规模为$K^{NM}$。现有的估计方法包括变分推导、随机模拟/采样、MCMC方法。其中变分推理是一个实现框架，具体而言有Loopy belief propagation方法和Mean field approximation方法。为了简单，以下VB即是说变分推理。 用最简单的话来讲，</p>
            </section>
        </a>
        <footer class="post-card-meta">
                <img class="author-profile-image" src="http://www.gravatar.com/avatar/731e3411d9e4ccc30af1dcd7c4c5af9f?s=250&amp;d=mm&amp;r=x" alt="zhaodaolimeng">
            <span class="post-card-author"><a href="../author/zhaodaolimeng/">zhaodaolimeng</a></span>
        </footer>
    </div>
</article>

                <article class="post-card post tag-java no-image">
    <div class="post-card-content">
        <a class="post-card-content-link" href="../qian-tan-java-io/">
            <header class="post-card-header">
                    <span class="post-card-tags">java</span>
                <h2 class="post-card-title">浅谈Java IO</h2>
            </header>
            <section class="post-card-excerpt">
                <p>简介 Java语言本身内置大量的业务模型，了解这些模型对于实际项目的开发很有帮助，Java IO 即为其中重要的一类。本文并非原创，是对有关的优秀博客资源的整理。Java IO中的流根据读入方式可分为字符流、字节流，按照功能又分为文件访问、访问数组、访问管道、缓冲、对象传递、打印、推回输入等。 功能\读入方式 字节流 字符流 抽象基类 InputStreamOutputStrea ReaderWriter 访问文件 FileInputStreamFileOutputStream FileReaderFileWriter 读写缓冲 BufferedInputStreamBufferedOutputStream BufferedReaderBufferedWriter 访问数组 ByteArrayInputStreamByteArrayOutputStream</p>
            </section>
        </a>
        <footer class="post-card-meta">
                <img class="author-profile-image" src="http://www.gravatar.com/avatar/731e3411d9e4ccc30af1dcd7c4c5af9f?s=250&amp;d=mm&amp;r=x" alt="zhaodaolimeng">
            <span class="post-card-author"><a href="../author/zhaodaolimeng/">zhaodaolimeng</a></span>
        </footer>
    </div>
</article>

        </div>
    </div>
</aside>

<div class="floating-header">
    <div class="floating-header-logo">
        <a href="../">
            <span>愚人拙记</span>
        </a>
    </div>
    <span class="floating-header-divider">—</span>
    <div class="floating-header-title">PRML.Ch9读书笔记：EM方法</div>
    <div class="floating-header-share">
        <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
    <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"></path>
</svg>
</div>
        <a class="floating-header-share-tw" href="https://twitter.com/share?text=PRML.Ch9%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%9AEM%E6%96%B9%E6%B3%95&amp;url=http://localhost:2368/prml-ch9du-shu-bi-ji-emfang-fa/" onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"></path></svg>
        </a>
        <a class="floating-header-share-fb" href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:2368/prml-ch9du-shu-bi-ji-emfang-fa/" onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"></path></svg>
        </a>
    </div>
    <progress class="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>
</div>




        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="../">愚人拙记</a> © 2017</section>
                <nav class="site-footer-nav">
                    <a href="../">Latest Posts</a>
                    
                    
                    <a href="https://ghost.org" target="_blank">Ghost</a>
                </nav>
            </div>
        </footer>

    </div>


    <script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="../assets/js/jquery.fitvids.js?v=f222684241"></script>


    <script>

// NOTE: Scroll performance is poor in Safari
// - this appears to be due to the events firing much more slowly in Safari.
//   Dropping the scroll event and using only a raf loop results in smoother
//   scrolling but continuous processing even when not scrolling
$(document).ready(function () {
    // Start fitVids
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    // End fitVids

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        // show/hide floating header
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>


    

</body>
