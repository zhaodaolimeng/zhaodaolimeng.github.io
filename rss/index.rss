<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" version="2.0"><channel><title>愚人拙记</title><description>Thoughts, stories and ideas.</description><link>http://localhost:2368/</link><image><url>http://localhost:2368/favicon.png</url><title>愚人拙记</title><link>http://localhost:2368/</link></image><generator>Ghost 1.7</generator><lastBuildDate>Sun, 03 Sep 2017 13:33:59 GMT</lastBuildDate><atom:link href="http://localhost:2368/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title>Snorkel学习笔记</title><description>&lt;div class="kg-card-markdown"&gt;&lt;h1 id=""&gt;简介&lt;/h1&gt;
&lt;p&gt;Snorkel是deepdive的后续项目，当前在github上很活跃。Snorkel将deepdive中的弱监督的思想进一步完善，使用纯python的形式构成了一套完整弱监督学习框架。&lt;/p&gt;
&lt;h1 id=""&gt;安装&lt;/h1&gt;
&lt;p&gt;由于Snorkel当前还处于开发状态，所以官方的安装教程不能保证在所有机器上都能顺利完成。&lt;br&gt;
官方使用了anaconda作为python支持环境，而这个环境在个人看来存在不少问题（在单位这个win 7机器上异常的慢）。&lt;br&gt;
我直接在一个ubuntu虚拟机内使用了virtualenv和原生python2.7对这套环境进行了安装。&lt;/p&gt;
&lt;h2 id="step1pythonvirtualenv"&gt;step 1: 部署python virtualenv&lt;/h2&gt;
&lt;p&gt;为了不污染全局python环境，官方推荐使用conda进行虚拟环境管理，这里使用了virtualenv。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt install python-pip python-dev essential-utils
pip install python-virtualenv
cd snorkel
virtualenv snorkel_env
source snorkel_env/bin/activate
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;根据官方文档安装依赖，并启用jupyter对应的功能。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install numba
pip install --requirement python-package-requirement.txt&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description><link>http://localhost:2368/snorkelxue-xi-bi-ji/</link><guid isPermaLink="false">59abf4fbd380da0cbf8294dc</guid><category>机器学习</category><category>知识抽取</category><dc:creator>zhaodaolimeng</dc:creator><pubDate>Sun, 03 Sep 2017 12:32:16 GMT</pubDate><content:encoded>&lt;div class="kg-card-markdown"&gt;&lt;h1 id=""&gt;简介&lt;/h1&gt;
&lt;p&gt;Snorkel是deepdive的后续项目，当前在github上很活跃。Snorkel将deepdive中的弱监督的思想进一步完善，使用纯python的形式构成了一套完整弱监督学习框架。&lt;/p&gt;
&lt;h1 id=""&gt;安装&lt;/h1&gt;
&lt;p&gt;由于Snorkel当前还处于开发状态，所以官方的安装教程不能保证在所有机器上都能顺利完成。&lt;br&gt;
官方使用了anaconda作为python支持环境，而这个环境在个人看来存在不少问题（在单位这个win 7机器上异常的慢）。&lt;br&gt;
我直接在一个ubuntu虚拟机内使用了virtualenv和原生python2.7对这套环境进行了安装。&lt;/p&gt;
&lt;h2 id="step1pythonvirtualenv"&gt;step 1: 部署python virtualenv&lt;/h2&gt;
&lt;p&gt;为了不污染全局python环境，官方推荐使用conda进行虚拟环境管理，这里使用了virtualenv。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt install python-pip python-dev essential-utils
pip install python-virtualenv
cd snorkel
virtualenv snorkel_env
source snorkel_env/bin/activate
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;根据官方文档安装依赖，并启用jupyter对应的功能。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install numba
pip install --requirement python-package-requirement.txt
jupyter nbextension enable --py widgetsnbextension --sys-prefix
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="step2jupyternotebook"&gt;step 2: 配置对虚拟机jupyter notebook的远程连接&lt;/h2&gt;
&lt;p&gt;jupyter notebook规定远程连接jupyter需要密码。&lt;br&gt;
使用以下命令生成密码的md5序列，放置到jupyter的配置文件中（也可以有其他生成密码的方式）。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;jupyter notebook generate-config
python -c 'from notebook.auth import passwd; print passwd()'
vim ~/.jupyter/jupyter_notebook_config.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;修改生成的jupyter配置文件。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;c.NotebookApp.ip = '*'
c.NotebookApp.password = u'sha1:bcd259ccf...&amp;lt;your hashed password here&amp;gt;'
c.NotebookApp.open_browser = False
c.NotebookApp.port = 8888
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这里的虚拟机我使用了virtualbox，所以还需要对nat的端口进行映射，外部host才能访问到虚拟机中运行的jupyter。&lt;/p&gt;
&lt;h2 id="step3"&gt;step 3: 启动项目&lt;/h2&gt;
&lt;p&gt;按理来说直接执行./run.sh即可，但我在这里执行之后set_env.sh没有生效，并且没有错误提示。遇到该情况后，单独执行了以下命令，手工实现了run.sh的功能。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;./install-parser.sh
unzip stanford-corenlp-full-2015-12-09.zip
mv stanford-corenlp-full-2015-12-09/stanford-corenlp-3.6.0.jar .
source set_env.sh
cd &amp;quot;$SNORKELHOME&amp;quot;
git submodule update --init --recursive
jupyter notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=""&gt;内部机制&lt;/h1&gt;
&lt;p&gt;与Deepdive的方式相同，一般使用一个外部库进行distance supervise，通过文本、文档结构等多种显性特征进行实体关系推断。&lt;br&gt;
开发者可以人工将这些显性特征进行深度的加工和组合，定义出不同的LF（Labeling Function）辅助特征提取，这对应了Deepdive中的UDF（User Defined Function）。&lt;/p&gt;
&lt;p&gt;还没有彻底弄清楚这里提到的GenerativeModel是否真的和传统意义上的生成模型是一回事？为什么感觉这里的Generative Model好像是一个以判别为目标的全新的模型？&lt;/p&gt;
&lt;h1 id=""&gt;用例&lt;/h1&gt;
&lt;h2 id="weathersentiment"&gt;Weather sentiment&lt;/h2&gt;
&lt;p&gt;对于tweet中的言论，已有少量的trusted work标注的信息和大量crowdsource的标注信息。&lt;br&gt;
Snorkel在这里使用时，将每个人的标注准确度作为一个随机变量，通过估计这些随机变量的分布，修正最终的标签。&lt;/p&gt;
&lt;h2 id="spouse"&gt;Spouse&lt;/h2&gt;
&lt;p&gt;这个任务和deepdive中的例程是相同的，都是通过一部分人工规则和辅助库，估计当前规则中是否存在问题，最终生成大量的可用样本。&lt;/p&gt;
&lt;h1 id=""&gt;总结&lt;/h1&gt;
&lt;p&gt;Deepdive和Snorkel都是从扩充训练样本的角度出发，提供了一套完整的通过专家知识生成训练数据的方法。但在整体任务的角度，如果采用这些原始标签，仍然利用这些（用于生成训练样本的）专家知识（以冲突消解的形式）生成特征、之后再使用模型进行集成训练，在理论上很可能与Deepdive和Snorkel的方法是等效的，这些都还有待验证。不管怎么说，Deepdive和Snorkel对于知识抽取这样的单一任务而言都是不错的工具。&lt;/p&gt;
&lt;/div&gt;</content:encoded></item><item><title>Deepdive学习笔记</title><description>&lt;div class="kg-card-markdown"&gt;&lt;h1 id="0"&gt;0 简介&lt;/h1&gt;
&lt;p&gt;deepdive是一个具有语言识别能力的信息抽取工具，可用作KBC系统（Knowledge Base Construction）的内核。&lt;br&gt;
也可以理解为是一种Automatic KBC工具。&lt;br&gt;
由于基于语法分析器构建，所以deepdive可通过各类文本规则实现实体间关系的抽取。&lt;br&gt;
deepdive面向异构、海量数据，所以其中涉及一些增量处理的机制。&lt;br&gt;
PaleoDeepdive是基于deepdive的一个例子，用于推测人、地点、组织之间的关系。&lt;br&gt;
deepdive的执行过程可以分为：feature extraction，probabilistic knowledge engineering，statistical inference and learning三部分。系统结构图如下所示：&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/09/2016-07-11-knowledge_base_construction_2.png" alt="2016-07-11-knowledge_base_construction_2"&gt;&lt;/p&gt;
&lt;p&gt;KBC系统中的四个主要概念：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Entity&lt;/li&gt;
&lt;li&gt;Relationship&lt;/li&gt;
&lt;li&gt;Mention，一段话中提及到某个实体或者关系了&lt;/li&gt;
&lt;li&gt;Relation Mention&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Deepdive的工作机制分为特征抽取、领域知识集成、监督学习、推理四步。&lt;br&gt;
闲话，Deepdive的作者之一Christopher &lt;a href="http://xn--ReLattice-w75noowmrzw83a5qcvwktqafmz48onzpgndc3o.io"&gt;Re之后创建了一个数据抽取公司Lattice.io&lt;/a&gt;，该公司在2017年3月份左右被苹果公司收购，用于改善Siri。&lt;/p&gt;
&lt;h1 id="1"&gt;1 安装&lt;/h1&gt;
&lt;h2 id="11cn_deepdive"&gt;1.&lt;/h2&gt;&lt;/div&gt;</description><link>http://localhost:2368/deepdivexue-xi-bi-ji/</link><guid isPermaLink="false">59abf228d380da0cbf8294d8</guid><category>机器学习</category><category>知识抽取</category><dc:creator>zhaodaolimeng</dc:creator><pubDate>Sat, 26 Aug 2017 08:32:00 GMT</pubDate><content:encoded>&lt;div class="kg-card-markdown"&gt;&lt;h1 id="0"&gt;0 简介&lt;/h1&gt;
&lt;p&gt;deepdive是一个具有语言识别能力的信息抽取工具，可用作KBC系统（Knowledge Base Construction）的内核。&lt;br&gt;
也可以理解为是一种Automatic KBC工具。&lt;br&gt;
由于基于语法分析器构建，所以deepdive可通过各类文本规则实现实体间关系的抽取。&lt;br&gt;
deepdive面向异构、海量数据，所以其中涉及一些增量处理的机制。&lt;br&gt;
PaleoDeepdive是基于deepdive的一个例子，用于推测人、地点、组织之间的关系。&lt;br&gt;
deepdive的执行过程可以分为：feature extraction，probabilistic knowledge engineering，statistical inference and learning三部分。系统结构图如下所示：&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/09/2016-07-11-knowledge_base_construction_2.png" alt="2016-07-11-knowledge_base_construction_2"&gt;&lt;/p&gt;
&lt;p&gt;KBC系统中的四个主要概念：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Entity&lt;/li&gt;
&lt;li&gt;Relationship&lt;/li&gt;
&lt;li&gt;Mention，一段话中提及到某个实体或者关系了&lt;/li&gt;
&lt;li&gt;Relation Mention&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Deepdive的工作机制分为特征抽取、领域知识集成、监督学习、推理四步。&lt;br&gt;
闲话，Deepdive的作者之一Christopher &lt;a href="http://xn--ReLattice-w75noowmrzw83a5qcvwktqafmz48onzpgndc3o.io"&gt;Re之后创建了一个数据抽取公司Lattice.io&lt;/a&gt;，该公司在2017年3月份左右被苹果公司收购，用于改善Siri。&lt;/p&gt;
&lt;h1 id="1"&gt;1 安装&lt;/h1&gt;
&lt;h2 id="11cn_deepdive"&gt;1.1 非官方中文版cn_deepdive&lt;/h2&gt;
&lt;p&gt;项目与文档地址：&lt;br&gt;
&lt;a href="http://openkg.cn/tool/cn-deepdive"&gt;http://openkg.cn/tool/cn-deepdive&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/qiangsiwei/DeepDive_Chinese"&gt;https://github.com/qiangsiwei/DeepDive_Chinese&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;使用了中文版本cndeepdive，&lt;a href="http://xn--openkg-hs4m556o.cn"&gt;来自openkg.cn&lt;/a&gt;。但这个版本已经老化，安装过程中有很多坑。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;moreutils无法安装问题。0.58版本失效，直接在deepdive/extern/bundled文件夹下的bundle.conf中禁用了moreutils工具，并在extern/.build中清理了对应的临时文件，之后使用了apt进行了安装。&lt;/li&gt;
&lt;li&gt;inference/dimmwitterd.sh中17行对g++版本检测的sed出现了问题，需要按照github上新版修改。&lt;/li&gt;
&lt;li&gt;numa.h: No such file or directory，直接安装libnuma-dev&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="12"&gt;1.2 官方版本&lt;/h2&gt;
&lt;p&gt;按照官方教程进行配置：&lt;a href="http://deepdive.stanford.edu/quickstart"&gt;http://deepdive.stanford.edu/quickstart&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ln -s articles-1000.tsv.bz2 input/articles.tsv.bz2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在input目录下有大量可用语料。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deepdive do articles
deepdive query '?- articles(&amp;quot;5beb863f-26b1-4c2f-ba64-0c3e93e72162&amp;quot;, content).' \
    format=csv | grep -v '^$' | tail -n +16 | head
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;使用Stanford CoreNLP对句子进行标注。包含NER、POS等操作。查询特定文档的分析结果：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;deepdive do sentences
deepdive query '?- 
    sentences(&amp;quot;5beb863f-26b1-4c2f-ba64-0c3e93e72162&amp;quot;, _, _, 
    tokens, _, _, ner_tags, _, _, _).' 
    format=csv | grep PERSON | tail
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看相关的命名实体对：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;deepdive do spouse_candidate
deepdive query 'name1, name2 ?-  
    spouse_candidate(p1, name1, p2, name2),  
    person_mention(p1, _, &amp;quot;5beb863f-26b1-4c2f-ba64-0c3e93e72162&amp;quot;, _, _, _).  
  '
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;运行查询：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deepdive do probabilities
deepdive sql &amp;quot;
    SELECT p1.mention_text, p2.mention_text, expectation
    FROM has_spouse_label_inference i, person_mention p1, person_mention p2
    WHERE p1_id LIKE '5beb863f-26b1-4c2f-ba64-0c3e93e72162%'
      AND p1_id = p1.mention_id AND p2_id = p2.mention_id
  &amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可视化展示：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mindbender search update
mindbender search gui
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id="2"&gt;2 工作机理&lt;/h1&gt;
&lt;p&gt;一个标准的KBC的运行机制：&lt;br&gt;
&lt;img src="http://localhost:2368/content/images/2017/09/image.png" alt="image"&gt;&lt;/p&gt;
&lt;p&gt;数据将首先进行实体识别，之后进行实体连接，最后输出满足用户指定关系的集合。&lt;br&gt;
例如，官方的has_couple这个例子中，输入是一个文档集合和一个小的人工标注的实例集合。&lt;br&gt;
系统输出是属于couple的list of pairs。&lt;/p&gt;
&lt;p&gt;Deepdive包含以下关键执行步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;特征抽取机制&lt;br&gt;
特征抽取是deepdive实现知识抽取的第一步。&lt;br&gt;
deepdive会使用Stanford NLPCore工具将所有的原始数据进行清洗，并每句一行存储于数据库中。&lt;br&gt;
这些原始特征内容包括：HTML标记、POS标签、语言标记。&lt;br&gt;
deepdive支持人工从原始数据中指定特定的pattern作为特征（所以需要专家撰写特征提取脚本，符合deepdive特征接口）。&lt;br&gt;
&lt;img src="http://localhost:2368/content/images/2017/09/fig2.2-image.png" alt="fig2.2-image"&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这些特征包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将某两个词在标签中是否是同一行作为特征&lt;/li&gt;
&lt;li&gt;语序结构上的特征&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些特征将用作之后自动推理的“证据”，用于推断不同实体之间的关系。&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;
&lt;p&gt;领域知识集成&lt;br&gt;
deepdive支持已知“事实”的输入，这些事实（专家知识）将在之后推理过程中对推理结果进行约束，可以改善结果。&lt;br&gt;
这些专家知识不是必须的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;监督学习&lt;br&gt;
很难找到足够的训练样本（用于训练特征组合）。&lt;br&gt;
deepdive里面有一种叫distant supervision的机制。&lt;br&gt;
其主体思想是在现有文本资料中人工找到具有正例的样本，并通过相关的关系生成反例样本。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;推理&lt;br&gt;
使用了factor graph进行对特征、知识的推理。&lt;br&gt;
factor graph在表示中分为User Schema和Correlation Schema两部分，定义为&lt;code&gt;$D=[R,F]$&lt;/code&gt;。&lt;br&gt;
以文本为例，factor graph将知识拆解为了随机变量和他们之间的关系。&lt;br&gt;
其中随机变量为不同位置的词语值，关系为词语位置、结构上的关系。&lt;br&gt;
Deepdive中的推理机制使用了Gibbs Sampling执行。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id="3deepdive"&gt;3 deepdive构成和程序使用&lt;/h1&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/09/fig3.1-image.png" alt="fig3.1-image"&gt;&lt;/p&gt;
&lt;p&gt;deepdive用户主要需要定义以下内容：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入数据的样式，包括关系样式、中间数据格式与数据之间的映射关系。&lt;/li&gt;
&lt;li&gt;用于进行distance supervise的规则。&lt;/li&gt;
&lt;li&gt;推理参数。&lt;/li&gt;
&lt;li&gt;原始数据处理方式。&lt;/li&gt;
&lt;li&gt;特征提取方式。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其中前三项内容由每个deepdive应用中的app.ddlog文件定义，后两项一般使用udf中的python或shell脚本实现。&lt;/p&gt;
&lt;h2 id="31ddlog"&gt;3.1 DDlog语言&lt;/h2&gt;
&lt;p&gt;DDlog是deepdive的专有语言，用于定义deepdive应用中的用户函数（UDF）和推理模型。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Q(x, y) :- R(x, y), S(y).
Q(x, y) :- T(x, y).
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;关系推理，表示Q可以由R推导出来（relation Q are derived from relations R and S）。此外，规则部分情况下可以进行简化，例如以上规则可以化简为。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Q(x, y) :- R(x, y), S(y); T(x, y).
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此外DDlog的文法支持常规逻辑运算和算术运算符，以及条件、表连接、聚合等SQL操作。&lt;/p&gt;
&lt;p&gt;&lt;a href="http://deepdive.stanford.edu/writing-dataflow-ddlog"&gt;http://deepdive.stanford.edu/writing-dataflow-ddlog&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="32udf"&gt;3.2 UDF用户自定义函数&lt;/h2&gt;
&lt;p&gt;UDF如果是python实现的话，将用annotation显示定义输入输出，同时需要在UDF中的添加对应的描述：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;function classify_articles over (id int, author text, words text[])
    returns (article_id int, topic text)
    implementation &amp;quot;udf/classify.py&amp;quot; handles tsj lines.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;对应的python程序将如下所示：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;#!/usr/bin/env python
from deepdive import *  # Required for @tsj_extractor and @returns

compsci_authors = [...]
bio_authors     = [...]
bio_words       = [...]

@tsj_extractor  # Declares the generator below as the main function to call
@returns(lambda # Declares the types of output columns as declared in DDlog
        article_id = &amp;quot;int&amp;quot;,
        topic      = &amp;quot;text&amp;quot;,
    :[])
def classify(   # The input types can be declared directly on each parameter as its default value
        article_id = &amp;quot;int&amp;quot;,
        author     = &amp;quot;text&amp;quot;,
        words      = &amp;quot;text[]&amp;quot;,
    ):
    &amp;quot;&amp;quot;&amp;quot;
    Classify articles by assigning topics.
    &amp;quot;&amp;quot;&amp;quot;
    num_topics = 0

    if author in compsci_authors:
        num_topics += 1
        yield [article_id, &amp;quot;cs&amp;quot;]

    if author in bio_authors:
        num_topics += 1
        yield [article_id, &amp;quot;bio&amp;quot;]
    elif any (word for word in bio_words if word in words):
        num_topics += 1
        yield [article_id, &amp;quot;bio&amp;quot;]

    if num_topics == 0:
        yield [article_id, None]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href="http://deepdive.stanford.edu/writing-udf-python"&gt;http://deepdive.stanford.edu/writing-udf-python&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</content:encoded></item><item><title>LDA模型入门</title><description>&lt;div class="kg-card-markdown"&gt;&lt;h1 id="1"&gt;1 引子&lt;/h1&gt;
&lt;p&gt;本文是对Blei等人LDA原始论文的总结。给定大量的文档，如何在无标注的情况下确定每个文档的主题词？LDA(Latent Dirichlet Allocation)是这类主题确定问题的一个成熟的解决方案。LDA最初面向文本挖掘领域，但随后在图像分类、行为识别等领域也得到了应用。LDA是一种典型的非监督模型，模型仅需要输入文档集合的词袋模型，模型可输出每个文档对应的主题，每个主题使用关键词的分布来表示。&lt;/p&gt;
&lt;h1 id="2"&gt;2 模型定义&lt;/h1&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/09/lda.png" alt="lda"&gt;&lt;/p&gt;
&lt;p&gt;LDA的PGM形式如上，我们认为主题数目有K个，文档有M个， 每个文档中有N个词。其中，$\alpha$ 是Dirichlet分布的参数，大小为1xK，用于控制生成主题的聚集程度； $\theta$ 表示一个文档中主题的分布大小为1xK； $z$ 为一个为每个词安排主题的01随机变量，大小为1xK，且只有一个值为1； $\beta$ 为一个多项分布的集合，大小为KxV，其中每一行代表一个主题中，不同词出现的概率；而w代表每个文档中的一个词。&lt;/p&gt;
&lt;p&gt;沿着上面的PGM的箭头方向，可以总结出词的生成过程。我们已知了每个文档中的词袋模型w，为了找到一组合适的主题，需要对分布 $p(w\vert\alpha,\beta)&lt;/p&gt;&lt;/div&gt;</description><link>http://localhost:2368/ldamo-xing-ru-men/</link><guid isPermaLink="false">59abf19dd380da0cbf8294d6</guid><category>机器学习</category><dc:creator>zhaodaolimeng</dc:creator><pubDate>Thu, 18 Aug 2016 12:52:00 GMT</pubDate><content:encoded>&lt;div class="kg-card-markdown"&gt;&lt;h1 id="1"&gt;1 引子&lt;/h1&gt;
&lt;p&gt;本文是对Blei等人LDA原始论文的总结。给定大量的文档，如何在无标注的情况下确定每个文档的主题词？LDA(Latent Dirichlet Allocation)是这类主题确定问题的一个成熟的解决方案。LDA最初面向文本挖掘领域，但随后在图像分类、行为识别等领域也得到了应用。LDA是一种典型的非监督模型，模型仅需要输入文档集合的词袋模型，模型可输出每个文档对应的主题，每个主题使用关键词的分布来表示。&lt;/p&gt;
&lt;h1 id="2"&gt;2 模型定义&lt;/h1&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/09/lda.png" alt="lda"&gt;&lt;/p&gt;
&lt;p&gt;LDA的PGM形式如上，我们认为主题数目有K个，文档有M个， 每个文档中有N个词。其中，$\alpha$ 是Dirichlet分布的参数，大小为1xK，用于控制生成主题的聚集程度； $\theta$ 表示一个文档中主题的分布大小为1xK； $z$ 为一个为每个词安排主题的01随机变量，大小为1xK，且只有一个值为1； $\beta$ 为一个多项分布的集合，大小为KxV，其中每一行代表一个主题中，不同词出现的概率；而w代表每个文档中的一个词。&lt;/p&gt;
&lt;p&gt;沿着上面的PGM的箭头方向，可以总结出词的生成过程。我们已知了每个文档中的词袋模型w，为了找到一组合适的主题，需要对分布 $p(w\vert\alpha,\beta)$ 进行推理。由于该分部中蕴含了隐变量主题 $\theta$ ，所以积分将 $\theta$ 积掉。代入Dirichlet分布 $p(\theta\vert\alpha)$ ，多项分布 $p(z_n\vert\theta)$ ，以及一个单独的概率值 $p(w_n\vert z_n,\beta)$ ，可得参数的后验概率形式。以下为完整的推导：&lt;/p&gt;
&lt;p&gt;$$p(w|\alpha,\beta) = \int p(\theta|\alpha)\prod_{n=1}^N p(w|\theta, \beta) d\theta$$&lt;br&gt;
$$= \int p(\theta|\alpha) (\prod_{n=1}^N \sum_{z_n}p(z_n|\theta)p(w_n|z_n,\beta))$$&lt;br&gt;
$$ = \frac{\Gamma(\sum_i\alpha_i)}{\prod_i{\Gamma(\alpha_i)}}\int(\prod_{i=1}^k\theta_i^{\alpha_i-1})(\prod_{n=1}^N\sum_{i=1}^k\prod_{j=1}^V(\theta_i\beta_{ij})^{w_n^j})d\theta$$&lt;/p&gt;
&lt;p&gt;模型的两个关键参数可以通过多种方法进行求解，即模型训练。&lt;/p&gt;
&lt;h1 id="3"&gt;3 模型训练&lt;/h1&gt;
&lt;h2 id="31"&gt;3.1 变分推理&lt;/h2&gt;
&lt;p&gt;Blei最初的LDA论文中，使用了变分推理（VB）求解LDA参数。这种方法试图使用一个不受约束的变分分布近似LDA的模型的联合概率。类似的手段可以参见Laplace近似，最经典的应用为使用高斯分布近似Bayesian Logistic Regression中观测的后验分布 $p(w\vert\bf{t})$ 。VB个人理解为一种链式的迭代估计框架。使用一个Q函数去近似真实分布函数。&lt;/p&gt;
&lt;h2 id="32gibbssampling"&gt;3.2 Gibbs Sampling&lt;/h2&gt;
&lt;p&gt;优势是便于编程实现。&lt;/p&gt;
&lt;h2 id="33"&gt;3.3 比较&lt;/h2&gt;
&lt;p&gt;变分推理的计算快于基于采样的方法，但可能会收敛到局部最优解。Matthew、Blei等人对于&lt;a href="http://papers.nips.cc/paper/3902-online-learning-for-latentdirichlet-allocation!"&gt;LDA在线学习&lt;/a&gt;&lt;br&gt;
中对变分推理进行了改进。采样方法更为直观、易于工程实现，且在多数场景下，采样的最终性能会好于变分推理。&lt;/p&gt;
&lt;h1 id="4"&gt;4 参考文献&lt;/h1&gt;
&lt;p&gt;&lt;a href="http://dl.acm.org/citation.cfm?id=944919.944937"&gt;Blei, David. Latent Dirichlet Allocation&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</content:encoded></item><item><title>PRML.Ch10读书笔记：变分推理</title><description>&lt;div class="kg-card-markdown"&gt;&lt;h1 id="0"&gt;0 疑问&lt;/h1&gt;
&lt;p&gt;这类概率推理问题在没有VB方法的时候都是怎么求解的？&lt;br&gt;
VB的直接好处是什么？&lt;br&gt;
什么是平均场估计？&lt;br&gt;
这里的估计方法和概率图中的BP的具体关系？&lt;br&gt;
VB中每一步的模型都是设定好的吗？例如LDA中使用Dirichlet作为后验概率？&lt;br&gt;
LDA中的VB是如何推导的？&lt;/p&gt;
&lt;h1 id="1"&gt;1 引子&lt;/h1&gt;
&lt;p&gt;本文是PRML第10章部分内容的摘录和总结。在很多概率问题中，如果使用精确求解，那么问题规模与随机变量的个数是指数上升的。以主题模型LDA为例，每个词的生成对应一个随机变量，使用确定性的方法会导致问题规模为$K^{NM}$。现有的估计方法包括变分推导、随机模拟/采样、MCMC方法。其中变分推理是一个实现框架，具体而言有Loopy belief propagation方法和Mean field approximation方法。为了简单，以下VB即是说变分推理。&lt;/p&gt;
&lt;p&gt;用最简单的话来讲，VB是说数据集中很多特性经过简单统计能反映原始参数的多少，每次迭代我们首先在E步对这些特性进行统计（实际上是求解充分统计量），之后，在M步看在这些统计结果的限制内，参数最可能是多少。这些特性可能是一些计数结果等，例如在LDA模型中，可能是属于不同主题的词的个数等等。有意思的是，在这个角度上VB方法与采样方法有着很大的相似点，唯一不同的是，VB方法每次迭代有明确的前进方向，而采样是靠数量取胜，从这里也能看出VB和采样的优势分别是速度和精度。&lt;/p&gt;
&lt;h1 id="2"&gt;2 核心思想&lt;/h1&gt;&lt;/div&gt;</description><link>http://localhost:2368/prml-ch10du-shu-bi-ji-bian-fen-tui-li/</link><guid isPermaLink="false">59abe825d380da0cbf8294d3</guid><category>机器学习</category><category>读书笔记</category><dc:creator>zhaodaolimeng</dc:creator><pubDate>Mon, 15 Aug 2016 11:38:00 GMT</pubDate><content:encoded>&lt;div class="kg-card-markdown"&gt;&lt;h1 id="0"&gt;0 疑问&lt;/h1&gt;
&lt;p&gt;这类概率推理问题在没有VB方法的时候都是怎么求解的？&lt;br&gt;
VB的直接好处是什么？&lt;br&gt;
什么是平均场估计？&lt;br&gt;
这里的估计方法和概率图中的BP的具体关系？&lt;br&gt;
VB中每一步的模型都是设定好的吗？例如LDA中使用Dirichlet作为后验概率？&lt;br&gt;
LDA中的VB是如何推导的？&lt;/p&gt;
&lt;h1 id="1"&gt;1 引子&lt;/h1&gt;
&lt;p&gt;本文是PRML第10章部分内容的摘录和总结。在很多概率问题中，如果使用精确求解，那么问题规模与随机变量的个数是指数上升的。以主题模型LDA为例，每个词的生成对应一个随机变量，使用确定性的方法会导致问题规模为$K^{NM}$。现有的估计方法包括变分推导、随机模拟/采样、MCMC方法。其中变分推理是一个实现框架，具体而言有Loopy belief propagation方法和Mean field approximation方法。为了简单，以下VB即是说变分推理。&lt;/p&gt;
&lt;p&gt;用最简单的话来讲，VB是说数据集中很多特性经过简单统计能反映原始参数的多少，每次迭代我们首先在E步对这些特性进行统计（实际上是求解充分统计量），之后，在M步看在这些统计结果的限制内，参数最可能是多少。这些特性可能是一些计数结果等，例如在LDA模型中，可能是属于不同主题的词的个数等等。有意思的是，在这个角度上VB方法与采样方法有着很大的相似点，唯一不同的是，VB方法每次迭代有明确的前进方向，而采样是靠数量取胜，从这里也能看出VB和采样的优势分别是速度和精度。&lt;/p&gt;
&lt;h1 id="2"&gt;2 核心思想&lt;/h1&gt;
&lt;p&gt;变分推理的最终目的是要找到一个形式简单的分布Q，使得其能较好地估计形式复杂的真实分布P的取值。当我们指定一个Q之后，可以列出Q与P的关系：&lt;/p&gt;
&lt;p&gt;$$\ln{p(\bf{X})}=\mathcal{L}(q)+KL(q||p)\qquad(1)$$&lt;/p&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;p&gt;$$\mathcal{L}(q)=\int{q(Z)\ln{\frac{p(X,Z)}{q(Z)}}dZ}$$&lt;/p&gt;
&lt;p&gt;$$KL(q||p)=-\int{q(Z)\ln{\frac{p{(Z|X)}}{q(Z)}}dZ}$$&lt;/p&gt;
&lt;p&gt;这里我们使用KL散度描述P与Q的近似程度。KL散度是似然比的对数期望，它也是确定q之后p的混乱程度。另外，由于因为q与p不同分布时   $KL(p\vert\vert q) \neq KL(q\vert\vert p)$ ，所以我们实际上面临 $KL(q\vert\vert p)$ 和 $KL(p\vert\vert q)$ 两个选择，实际情况是前者更为合理。如果我们能获得$Z$的解析形式的关系，那么参照EM方法中迭代求解隐变量的思路，即可求解隐变量的随机分布。VB与EM的最大区别在于VB中不再出现模型参数，取而代之的是随机变量。&lt;/p&gt;
&lt;h2 id="21klqvertvertp"&gt;2.1 为何使用 $KL(q\vert\vert p)$&lt;/h2&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/09/em.png" alt="em"&gt;&lt;/p&gt;
&lt;p&gt;$KL(q\vert\vert p)$ 更倾向于使 $q$ 去精确拟合 $p$ 概率密度为0时的位置，这就导致对于分离的概率密度函数，$q$ 会产生一种聚集效果，即像后两个图一样拟合其中一个分离的分布，而不是像(a)一样试图拟合非0位置，这种行为叫做model-seeking。&lt;/p&gt;
&lt;h2 id="22q"&gt;2.2 分布Q的合理形式&lt;/h2&gt;
&lt;p&gt;这种合理形式叫做可分解分布，满足：&lt;/p&gt;
&lt;p&gt;$$q(Z)=\prod_{i=1}^{M} q_i(Z_i)$$&lt;/p&gt;
&lt;p&gt;使用这种假设的好处是可将原始分布分解为多个较低维度的成分，可简化计算，这种方法在统计物理中被称为平均场方法。回顾公式(1)，我们的VB的最终目标是求一个Q，使得Q与P的KL距离最小，这等价于 $\mathcal{L}(q)$ 的最大化。事实上，由(1)式可直接获得如下关系：&lt;/p&gt;
&lt;p&gt;$$\mathcal{L}(q)=\int{\ln{p(X,Z)}-\sum_{i}{\ln{q_i}}}\prod_{i}{q_i(x_i)}dZ$$&lt;br&gt;
$$=\int q_{j}\ln{\tilde{p}(X,Z_j)dZ_j}-\int{q_j\ln q_j}dZ_j+\text{const}$$&lt;/p&gt;
&lt;p&gt;以上公式是为了获得 $q_j$ 和其他 $q$ 的关系，以析解得目标(1)的最优解。推导过程中注意积分变量和提出被积变量中的常量。回顾公式(1)，我们令KL散度直接为0使 $\mathcal{L}(q)=\ln(p)$，可得以下公式：&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\ln{q^\star_{j_{(\bf{Z_j})}}}=\mathbb{E}_{i\neq{j}}\ln{p(\bf{X},\bf{Z})}]+\text{const}\qquad(2)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;结论就是：为了估计随机变量$q_j$的分布，需要对其他所有随机变量的求期望，这样就极小化了KL散度，即使得Q与P更为接近。&lt;/p&gt;
&lt;h1 id="3"&gt;3 实例&lt;/h1&gt;
&lt;p&gt;VB方法具有一个统一的推导求解框架，但对于不同的模型往往会有不同的insight，PRML中也从不同的方向进行了求解。&lt;/p&gt;
&lt;h2 id="31"&gt;3.1 二元高斯模型&lt;/h2&gt;
&lt;p&gt;（待补充）&lt;/p&gt;
&lt;h2 id="32"&gt;3.2 混合高斯模型&lt;/h2&gt;
&lt;p&gt;首先将GMM模型进行贝叶斯化，GMM的生成模型如下：&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\alpha_0 \rightarrow \pi \rightarrow Z \rightarrow X \leftarrow \mu,\Lambda&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;其中，X为观测变量，大小为1*N；Z为每个观测变量在不同类别中的归属，使用01表示，大小为是K*N；$\pi$ 为不同类别的权重，大小为1*K； $\alpha_0$ 为决定 $\pi$ 形态的超参数，大小为1*K； $\mu$和 $\Lambda$ 本身为每个正态分量的均值和方差参数。其中，变量间的关系如下：&lt;/p&gt;
&lt;p&gt;$$p(X|Z,\mu,\Lambda)=\prod_{n=1}^N\prod_{k=1}^K\mathcal{N}(x_n|\mu_k,\Lambda_k^{-1})^{z_{nk}}$$&lt;/p&gt;
&lt;p&gt;$$p(Z|\pi) = \prod_{n=1}^{N}\prod_{k=1}^K\pi_{k}^{z_{nk}}$$&lt;/p&gt;
&lt;p&gt;$$p(\pi)=\text{Dir}(\pi|\alpha_0)=C(\alpha_0)\prod_{k=1}^K{\pi_k^{\alpha_0-1}}$$&lt;/p&gt;
&lt;p&gt;$$p(\mu,\Lambda)=\prod_{k=1}^{K}{\mathcal{N}(\mu_k|m_0,(\beta_0\Lambda_kk)^{-1})\mathcal{W}(\Lambda_k|W_0,v_0)}$$&lt;/p&gt;
&lt;p&gt;可以看出 $p(Z\vert\pi)$ 是以 $\pi$ 为参数的多项分布。 $p(\pi\vert\alpha_0)$ 可使用Dirichlet分布进行描述，正态分布的参数可使用Gaussian-Wishart分布描述，因为他们分别是多项分布和高斯分布的先验共轭。&lt;/p&gt;
&lt;p&gt;最终目标实际上是估计以上关系构成的联合分布  $p(X,Z,\pi,\mu,\Lambda)$ 。我们使用2.2节中提到的可分解分布  $q(Z)q(\pi,\mu,\Lambda)$ 对 $p$ 进行估计。一种直观的方式是分解整个联合概率分布，以构造分布Q，如下：&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\ln q^\star(Z)=\mathbb{E}_{\pi,\mu,\Lambda}[\ln{p(X,Z,\pi,\mu,\Lambda)}] + \text{const}\quad(3)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;将 $p$ 的生成模型代入其中，直接将(3)展开，可得到一种分布分解的情况。&lt;/p&gt;
&lt;p&gt;$$\ln q^\star(Z)=\sum_{n=1}^N\sum_{k=1}^K z_{nk} \ln{r_{nk}}$$&lt;/p&gt;
&lt;p&gt;这里没有将 $r$ 进行展开，其意义为Q函数中一个规范化后的 $q$ 。观察到 $r$ 的形式和我们想要的Q函数的形式一致，所以我们直接令 $\mathbb{E}[z_{nk}]= r_{nk}$ 可得到VB的参数推导公式。但遗憾的是这种方法得到的结果与MLE是一致的，而事实证明MLE方法在对应GMM模型中有较为明显的缺陷，即从整个联合概率密度进行估计Q函数并不是一个好的方式。&lt;/p&gt;
&lt;p&gt;一个更合适的方式是尝试将 $\ln{q^\star}(\pi,\mu,\Lambda)$ 进行分布的分解。E步使用期望估计Q函数的形式，待估计函数的展开形式仍然如(3)所示，该步骤可产生关键的模型参数 $r_{nk}$ ：&lt;br&gt;
$$&lt;br&gt;
\mathbb{E}_{\mu_k,\Lambda_k}[(x_n-\mu_k)^T\Lambda_k(x_n-\mu_k)]=D\beta_k^{-1} + v_k(x_n-m_k)^TW_k(x_n-m_k)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\ln{\tilde\Lambda_k}=\mathbb{E}[\ln|\Lambda_k|]=\sum_{i=1}^D\psi(\frac{v_k+1-i}{2})+D\ln 2 +\ln|W_k|&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\ln\tilde\pi_k=\mathbb{E}[\ln\pi_k]=\psi(\alpha_k)-\psi(\sum_k\alpha_k)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;以上公式带入(3)中可获得：&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
r_{nk}\propto \tilde\pi_k\tilde\Lambda_k^{1/2}\exp{-\frac{D}{2\beta_k}-\frac{v_k}{2}(x_n-m_k)^T W_k(x_n-m_k)}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;M步更新原始模型， $r_{nk}$ 为计算原始模型的参数：&lt;/p&gt;
&lt;p&gt;$$q^\star(\pi)=\text{Dir}(\pi|\alpha)$$&lt;/p&gt;
&lt;p&gt;$$q^\star(\mu_k,\Lambda_k)=\mathcal{N}(\mu_k|m_k,(\beta_k\Lambda_k)^{-1})\mathcal{W}(\Lambda_k|W_k,v_k)$$&lt;/p&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;p&gt;$$\beta_k=\beta_0+N_k$$&lt;br&gt;
$$m_k=\frac{1}{\beta_k}(\beta_0 m_0 + N_k \bar{x}_k)$$&lt;br&gt;
$$W_k^{-1}=W_0^{-1}+N_k S_k + \frac{\beta_0 N_k}{\beta_0+N_k}(\bar x_k - m_0)(\bar x_k - m_0)^T$$&lt;br&gt;
$$v_k = v_0 + N_k$$&lt;/p&gt;
&lt;p&gt;以上分布分解方法最终得到的Q函数计算结果与EM方法是一致的。&lt;/p&gt;
&lt;h2 id="33"&gt;3.3 贝叶斯线性回归&lt;/h2&gt;
&lt;p&gt;（待补充）&lt;/p&gt;
&lt;h1 id="4"&gt;4 参考文献&lt;/h1&gt;
&lt;p&gt;&lt;a href="http://www.blog.huajh7.com/variational-bayes/"&gt;中文博客huajh7, Variational Bayes&lt;/a&gt;&lt;br&gt;
&lt;a href="http://www.52nlp.cn/prml%E8%AF%BB%E4%B9%A6%E4%BC%9A%E7%AC%AC%E5%8D%81%E7%AB%A0-approximate-inference"&gt;PRML读书会第十章&lt;/a&gt;&lt;br&gt;
&lt;a href="http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf"&gt;C. Bishop, PRML Chapter10&lt;/a&gt;&lt;br&gt;
&lt;a href="https://www.cs.cmu.edu/~epxing/Class/10708-14/lecture.html"&gt;E. Xing, Probabilistic Graphical Models&lt;/a&gt;&lt;br&gt;
&lt;a href="http://www.cse.buffalo.edu/faculty/mbeal/thesis/"&gt;B. Matthew, PhD thesis&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</content:encoded></item><item><title>PRML.Ch9读书笔记：EM方法</title><description>&lt;div class="kg-card-markdown"&gt;&lt;h1 id="1"&gt;1 引子&lt;/h1&gt;
&lt;p&gt;本文涉及EM的使用场景区别、理论推导。以实践的角度，EM方法是一个迭代优化以求解隐变量的方法。本文内容是PRML第九章的缩减。&lt;/p&gt;
&lt;h1 id="2emgmm"&gt;2 EM用于GMM模型&lt;/h1&gt;
&lt;h2 id="21gmm"&gt;2.1 极大似然尝试求解GMM参数&lt;/h2&gt;
&lt;p&gt;以GMM为例，这里先试图使用最大似然估计的方式求解参数：&lt;br&gt;
$$&lt;br&gt;
p(x|\pi,\mu,\Sigma)=\sum_{k=1}^{K}\pi_{k}\mathcal{N}(x|\mu_{k},\Sigma_{k})&lt;br&gt;
$$&lt;br&gt;
最终目标是求解两部分内容：未观测变量和模型参数。个人理解对于GMM，其未观测变量可明确地指定为 $\pi_{k}$，而其模型参数确定为 $\mu_k$和 $\Sigma_k$。这里优化目标是当前的估计导致的损失，或者说对数似然函数：&lt;/p&gt;
&lt;p&gt;$$\ln{p(X|\pi,&lt;/p&gt;&lt;/div&gt;</description><link>http://localhost:2368/prml-ch9du-shu-bi-ji-emfang-fa/</link><guid isPermaLink="false">59abe3a4d380da0cbf8294cc</guid><category>机器学习</category><category>读书笔记</category><dc:creator>zhaodaolimeng</dc:creator><pubDate>Fri, 12 Aug 2016 12:52:00 GMT</pubDate><content:encoded>&lt;div class="kg-card-markdown"&gt;&lt;h1 id="1"&gt;1 引子&lt;/h1&gt;
&lt;p&gt;本文涉及EM的使用场景区别、理论推导。以实践的角度，EM方法是一个迭代优化以求解隐变量的方法。本文内容是PRML第九章的缩减。&lt;/p&gt;
&lt;h1 id="2emgmm"&gt;2 EM用于GMM模型&lt;/h1&gt;
&lt;h2 id="21gmm"&gt;2.1 极大似然尝试求解GMM参数&lt;/h2&gt;
&lt;p&gt;以GMM为例，这里先试图使用最大似然估计的方式求解参数：&lt;br&gt;
$$&lt;br&gt;
p(x|\pi,\mu,\Sigma)=\sum_{k=1}^{K}\pi_{k}\mathcal{N}(x|\mu_{k},\Sigma_{k})&lt;br&gt;
$$&lt;br&gt;
最终目标是求解两部分内容：未观测变量和模型参数。个人理解对于GMM，其未观测变量可明确地指定为 $\pi_{k}$，而其模型参数确定为 $\mu_k$和 $\Sigma_k$。这里优化目标是当前的估计导致的损失，或者说对数似然函数：&lt;/p&gt;
&lt;p&gt;$$\ln{p(X|\pi,\mu,\Sigma)}=&lt;br&gt;
\sum_{n=1}^{N}{\ln\sum_{k=1}^K{\pi_k\mathcal{N}(x_n|\mu_k,\Sigma_k)}}$$&lt;/p&gt;
&lt;p&gt;以上问题由于隐变量的存在，同时由于参数在正态分布的积分中，一般来说是难解的。具体地，对 $\ln{p(X|\pi,\mu,\Sigma)}$ 求导，并令导数为0可以看出隐变量和参数之间的关系：&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\frac{\partial{\ln{p(X|\pi,\mu,\Sigma)}}}{\partial{\mu_k}}&lt;br&gt;
=-\sum_{n=1}^{N} \gamma(z_{nk})\Sigma_k(x_n-\mu_k)=0&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\frac{\partial{\ln{p(X|\pi,\mu,\Sigma)}}}{\partial{\Sigma_k}}&lt;br&gt;
=\sum_{n=1}^N \gamma(z_{nk}) {&lt;br&gt;
-\frac{N}{2}\Sigma^{-1}+\frac{N}{2}\Sigma^{-1}\sum_{d=1}^{D}(x_i-\mu)^T \Sigma^{-1}_k (x_i-\mu)\Sigma^{-1}&lt;br&gt;
} =0&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;其中，$\gamma(z_{nk})$ 的物理意义是第n个观测在第k簇的概率，形式为：&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\gamma(z_{nk})=\frac{\pi_k\mathcal{N}(x_n|\mu_k,\Sigma_k)}{\sum_j{\pi_j\mathcal{N}(x_n|\mu_j,\Sigma_j)}}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;具体的结果可参考PRML。使用以上两个等式，原则上可计算参数和未观测量的值，这里是为了展现：由于对数中本身有加和的形式，这种方式难以获得解析解。需要有一个更便捷的框架解决以上参数求解问题。&lt;/p&gt;
&lt;h2 id="22emgmm"&gt;2.2 EM方法估计GMM参数&lt;/h2&gt;
&lt;p&gt;EM方法正是这样一个框架：套用以上的结果，使用迭代的方法通过不断修正找到一个函数$q(x)$ ，使得$q(x)$ 与$p(x)$ 接近，那么即可使用$q(x)$ 对最终结果进行近似。具体的步骤如下：&lt;/p&gt;
&lt;p&gt;(1) 初始化参数 $\mu_k$、$\Sigma_k$和未观测值 $\pi_k$。一个可行的方式是，由于K-means迭代次数较快，可使用K-means对数据进行预处理，然后选择K-means的中心点作为 $\mu_k$的初值。&lt;br&gt;
(2) E步，固定模型参数，优化未观测变量：&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\gamma(z_{nk})=\frac{\pi_k\mathcal{N}(x_n|\mu_k,\Sigma_k)}{\sum_j{\pi_j\mathcal{N}(x_n|\mu_j,\Sigma_j)}}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;(3) M步，M步将固定未观测变量，优化模型参数：&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mu_k^{new}=\frac{1}{N_k}\sum_{n=1}^{N}\gamma(z_{nk})\bf{x}_n&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\Sigma_k^{new}=\frac{1}{N_k}\sum_{n=1}^{N}\gamma(z_{nk})(\bf{x}_n-\mu_k^{new})(\bf{x}_n-\mu_k^{new})^T&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;(4) 计算likehood，如果结果收敛则停止。&lt;/p&gt;
&lt;h1 id="3em"&gt;3 EM方法正确性&lt;/h1&gt;
&lt;p&gt;（待续）&lt;/p&gt;
&lt;/div&gt;</content:encoded></item><item><title>浅谈Java IO</title><description>&lt;div class="kg-card-markdown"&gt;&lt;h1 id=""&gt;简介&lt;/h1&gt;
&lt;p&gt;Java语言本身内置大量的业务模型，了解这些模型对于实际项目的开发很有帮助，Java IO 即为其中重要的一类。本文并非原创，是对有关的优秀博客资源的整理。Java IO中的流根据读入方式可分为字符流、字节流，按照功能又分为文件访问、访问数组、访问管道、缓冲、对象传递、打印、推回输入等。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align:left"&gt;功能\读入方式&lt;/th&gt;
&lt;th style="text-align:right"&gt;字节流&lt;/th&gt;
&lt;th style="text-align:right"&gt;字符流&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align:left"&gt;抽象基类&lt;/td&gt;
&lt;td style="text-align:right"&gt;InputStream&lt;br&gt;OutputStrea&lt;/td&gt;
&lt;td style="text-align:right"&gt;Reader&lt;br&gt;Writer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left"&gt;访问文件&lt;/td&gt;
&lt;td style="text-align:right"&gt;FileInputStream&lt;br&gt;FileOutputStream&lt;/td&gt;
&lt;td style="text-align:right"&gt;FileReader&lt;br&gt;FileWriter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left"&gt;读写缓冲&lt;/td&gt;
&lt;td style="text-align:right"&gt;BufferedInputStream&lt;br&gt;BufferedOutputStream&lt;/td&gt;
&lt;td style="text-align:right"&gt;BufferedReader&lt;br&gt;BufferedWriter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left"&gt;访问数组&lt;/td&gt;
&lt;td style="text-align:right"&gt;ByteArrayInputStream&lt;br&gt;ByteArrayOutputStream&lt;/td&gt;
&lt;td style="text-align:right"&gt;CharArrayReader&lt;br&gt;CharArrayWriter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left"&gt;访问管道&lt;/td&gt;
&lt;td style="text-align:right"&gt;PipedInputStream&lt;br&gt;PipedOutputStream&lt;/td&gt;
&lt;td style="text-align:right"&gt;PipedReader&lt;br&gt;PipedWriter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left"&gt;访问字符串&lt;/td&gt;
&lt;td style="text-align:right"&gt;&lt;/td&gt;
&lt;td style="text-align:right"&gt;StringReader&lt;br&gt;StringWriter&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;</description><link>http://localhost:2368/qian-tan-java-io/</link><guid isPermaLink="false">59abe06ef59f8d07cf681657</guid><category>java</category><dc:creator>zhaodaolimeng</dc:creator><pubDate>Fri, 10 Jun 2016 11:41:00 GMT</pubDate><content:encoded>&lt;div class="kg-card-markdown"&gt;&lt;h1 id=""&gt;简介&lt;/h1&gt;
&lt;p&gt;Java语言本身内置大量的业务模型，了解这些模型对于实际项目的开发很有帮助，Java IO 即为其中重要的一类。本文并非原创，是对有关的优秀博客资源的整理。Java IO中的流根据读入方式可分为字符流、字节流，按照功能又分为文件访问、访问数组、访问管道、缓冲、对象传递、打印、推回输入等。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align:left"&gt;功能\读入方式&lt;/th&gt;
&lt;th style="text-align:right"&gt;字节流&lt;/th&gt;
&lt;th style="text-align:right"&gt;字符流&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align:left"&gt;抽象基类&lt;/td&gt;
&lt;td style="text-align:right"&gt;InputStream&lt;br&gt;OutputStrea&lt;/td&gt;
&lt;td style="text-align:right"&gt;Reader&lt;br&gt;Writer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left"&gt;访问文件&lt;/td&gt;
&lt;td style="text-align:right"&gt;FileInputStream&lt;br&gt;FileOutputStream&lt;/td&gt;
&lt;td style="text-align:right"&gt;FileReader&lt;br&gt;FileWriter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left"&gt;读写缓冲&lt;/td&gt;
&lt;td style="text-align:right"&gt;BufferedInputStream&lt;br&gt;BufferedOutputStream&lt;/td&gt;
&lt;td style="text-align:right"&gt;BufferedReader&lt;br&gt;BufferedWriter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left"&gt;访问数组&lt;/td&gt;
&lt;td style="text-align:right"&gt;ByteArrayInputStream&lt;br&gt;ByteArrayOutputStream&lt;/td&gt;
&lt;td style="text-align:right"&gt;CharArrayReader&lt;br&gt;CharArrayWriter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left"&gt;访问管道&lt;/td&gt;
&lt;td style="text-align:right"&gt;PipedInputStream&lt;br&gt;PipedOutputStream&lt;/td&gt;
&lt;td style="text-align:right"&gt;PipedReader&lt;br&gt;PipedWriter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left"&gt;访问字符串&lt;/td&gt;
&lt;td style="text-align:right"&gt;&lt;/td&gt;
&lt;td style="text-align:right"&gt;StringReader&lt;br&gt;StringWriter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left"&gt;转换流&lt;/td&gt;
&lt;td style="text-align:right"&gt;&lt;/td&gt;
&lt;td style="text-align:right"&gt;InputStreamReader&lt;br&gt;OutputStreamWriter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left"&gt;对象流&lt;/td&gt;
&lt;td style="text-align:right"&gt;ObjectInputStream&lt;br&gt;ObjectOutputStream&lt;/td&gt;
&lt;td style="text-align:right"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left"&gt;打印流&lt;/td&gt;
&lt;td style="text-align:right"&gt;PrintStream&lt;/td&gt;
&lt;td style="text-align:right"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left"&gt;推回输入流&lt;/td&gt;
&lt;td style="text-align:right"&gt;PushbackInputStream&lt;/td&gt;
&lt;td style="text-align:right"&gt;PushbackReader&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left"&gt;特殊流&lt;/td&gt;
&lt;td style="text-align:right"&gt;DataInputStream&lt;br&gt;DataOutputStream&lt;/td&gt;
&lt;td style="text-align:right"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id="inputstream"&gt;InputStream&lt;/h1&gt;
&lt;p&gt;&lt;img src="https://lh3.googleusercontent.com/-Kwc5u5HtGy4/Vy3lexCUOmI/AAAAAAAAAl0/zASQs313j20UaAY290NAgm1GfrN_tKiLwCLcB/s0/java.io.InputStream.gif" alt="输入字节流" title="java.io.InputStream.gif"&gt;&lt;/p&gt;
&lt;p&gt;FileInputStream仅支持简单的read()和read(byte[])等方法。ByteArrayInputStream提供一种从字符序列转换成流的手段，例如：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-java"&gt;byte[] bytes = &amp;quot;Hi!&amp;quot;.getBytes(Charsets.UTF_8) //get byte array from somewhere.
InputStream input = new ByteArrayInputStream(bytes);
int data = input.read();
while(data != -1) {
  //do something with data
  data = input.read();
}
input.close();    
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;PipedInputStream以及下文的PipedOutputStream实现了IPC的管道通讯。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-java"&gt;final PipedOutputStream output = new PipedOutputStream();
final PipedInputStream  input  = new PipedInputStream(output);

Thread thread1 = new Thread(new Runnable() {
    @Override
    public void run() {
        try {
            output.write(&amp;quot;Hello world, pipe!&amp;quot;.getBytes());
        } catch (IOException e) {
        }
    }
});
Thread thread2 = new Thread(new Runnable() {
    @Override
    public void run() {
        try {
            int data = input.read();
            while(data != -1){
                System.out.print((char) data);
                data = input.read();
            }
        } catch (IOException e) {
        }
    }
});
thread1.start();
thread2.start();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;SequenceInputStream用以将两个或多个流合成一个先后输出的流。ObjectInputStream将一个InputStream读取成Object的形式。FilterInputStream使用了装饰器模式，为输入的流增加额外的功能，是几类常用流类型的基类。其中，BufferedInputStream提供缓冲功能，用户可以指定缓冲队列的大小。PushbackInputStream提供一种额外的从流对象中预读的机制，特有unread函数。DataInputStream是最常用的流类型，提供readChar、readInt等函数（没有readLine），可以从一个原始流中读取多种不同数据类型的数据。&lt;/p&gt;
&lt;h1 id="outputstream"&gt;OutputStream&lt;/h1&gt;
&lt;p&gt;&lt;img src="https://lh3.googleusercontent.com/-b6BA3eto32w/Vy3tmHYa3zI/AAAAAAAAAmg/rU1_QEURadEL3jbVAog_vWDzRfqNhdnpgCLcB/s0/java.io.OutputStream.gif" alt="输出字节流" title="java.io.OutputStream.gif"&gt;&lt;/p&gt;
&lt;p&gt;OutputStream与InputStream严格对应，唯一不同的是PrintStream。实际使用上类似于DataOutputStream，支持将不同类型的数据写入OutputStream实例中，但后者直接输出二进制，前者输出字符形式，故前者更常用于显示输出方面。&lt;/p&gt;
&lt;h1 id="reader"&gt;Reader&lt;/h1&gt;
&lt;p&gt;&lt;img src="https://lh3.googleusercontent.com/-zlJ6I4DOET8/Vy3lsCzELxI/AAAAAAAAAl8/VeHDkmDrL28CbbvG6Z7PyfNsXIRTY5gCQCLcB/s0/java.io.Reader.gif" alt="输入字符流" title="java.io.Reader.gif"&gt;&lt;/p&gt;
&lt;p&gt;InputStreamReader提供了与FileInputStream完全一致的函数调用方法，但不能直接打开文件，它最常用的场景是将一个InputStream对象转换成一个Reader对象。FileReader可以直接打开一个文件，且本身为一个Reader对象。其他的各个Reader根据名称，功能与InputStream类似。总体而言，Reader提供了更丰富的处理字符IO的操作，如以下一段常用的文件操作代码：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-java"&gt;FileInputStream is = new FileInputStream(&amp;quot;file.txt&amp;quot;);
BufferedReader br = new BufferedReader(new InputStreamReader(is));
StringBuilder sb = new StringBuilder();
String line;
while ((line = br.readLine()) != null)
    sb.append(line);
System.out.println(sb.toString());
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;StringReader提供了一种从String类型转换到Reader的快捷的方式（否则要先转换成ByteArrayInputStream）。LineNumberReader提供特殊方法getLineNumber()。&lt;/p&gt;
&lt;h1 id="writer"&gt;Writer&lt;/h1&gt;
&lt;p&gt;&lt;img src="https://lh3.googleusercontent.com/-rGsX5JvzpiU/Vy3wDIZPtkI/AAAAAAAAAmw/-I6iK4__WsEjnM8K5MeP6CygXVZB5E1lQCLcB/s0/java.io.Writer.gif" alt="输出字符流" title="java.io.Writer.gif"&gt;&lt;/p&gt;
&lt;p&gt;这里PrintWriter与PrintStream非常类似，但PrintStream使用默认系统编码进行输出，而对于一个Writer对象，可以手动指定不同的编码形式，在跨平台时更为安全。&lt;/p&gt;
&lt;h1 id=""&gt;其他&lt;/h1&gt;
&lt;p&gt;&lt;img src="https://lh3.googleusercontent.com/-diA3wMOD8-g/Vy3wS1CCLBI/AAAAAAAAAm4/289o_WH6lhI6nfi5B5Fn9EMBLqcZGaXqwCLcB/s0/java.io.Misc.gif" alt="enter image description here" title="java.io.Misc.gif"&gt;&lt;/p&gt;
&lt;p&gt;Java IO包中还包括StreamTokenizer，用于将一个文件的输入以空格、空白为划分识别为不同token。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-java"&gt;StreamTokenizer st= new StreamTokenizer(
        new StringReader(&amp;quot;Mary had 1 little lamb...&amp;quot;));
while(st.nextToken() != StreamTokenizer.TT_EOF){
    if(st.ttype == StreamTokenizer.TT_WORD)
        System.out.println(st.sval);
    else if(st.ttype == StreamTokenizer.TT_NUMBER)
        System.out.println(st.nval);
    else if(st.ttype == StreamTokenizer.TT_EOL)
        System.out.println();    
}
st.close();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;另外，RandomAccessFile提供了一种类似于C语言的文件处理方式，具有seek()方法，而这在常用文件对象File中是没有的。&lt;/p&gt;
&lt;h1 id=""&gt;参考文献&lt;/h1&gt;
&lt;p&gt;&lt;a href="http://tutorials.jenkov.com/java-io/index.html"&gt;http://tutorials.jenkov.com/java-io/index.html&lt;/a&gt;&lt;br&gt;
&lt;a href="http://lcy0202.iteye.com/blog/1562339"&gt;http://lcy0202.iteye.com/blog/1562339&lt;/a&gt;&lt;br&gt;
&lt;a href="http://www.falkhausen.de/en/diagram/html/java.io.InputStream.html"&gt;http://www.falkhausen.de/en/diagram/html/java.io.InputStream.html&lt;/a&gt;&lt;br&gt;
&lt;a href="http://www.mkyong.com/java/how-to-convert-inputstream-to-file-in-java/"&gt;http://www.mkyong.com/java/how-to-convert-inputstream-to-file-in-java/&lt;/a&gt;&lt;br&gt;
&lt;a href="https://stackoverflow.com/questions/2822005/java-difference-between-printstream-and-printwriter"&gt;https://stackoverflow.com/questions/2822005/java-difference-between-printstream-and-printwriter&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</content:encoded></item><item><title>Nimbits：一个IoT数据汇集平台</title><description>&lt;div class="kg-card-markdown"&gt;&lt;h1 id="1"&gt;1. 简介&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/bsautner/com.nimbits"&gt;Nimbits&lt;/a&gt;是一个专门针对物联网设计的数据汇集平台。该平台使用嵌入的hsql作为数据存储，使用Java作为编程语言，运行于Web环境中。相对于现有的Google/IBM 等公司的用于IoT的云平台，该平台简单、灵活、高度可定制，可为小型IoT业务提供了一套完整的数据解决方案。本文将介绍如何在本地搭建Appscale集群，并于之上部署Nimbits。&lt;/p&gt;
&lt;h1 id="2"&gt;2. 运行环境搭建&lt;/h1&gt;
&lt;p&gt;需要先后安装Virtual Box、Vagrant、Appscale。安装流程解释请参照&lt;br&gt;
&lt;a href="http://www.appscale.com/faststart"&gt;fast-install&lt;/a&gt;和&lt;a href="https://github.com/AppScale/appscale/wiki/AppScale-on-VirtualBox"&gt;Appscale wiki&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id="virtualboxvagrant"&gt;安装VirtualBox和Vagrant&lt;/h2&gt;
&lt;p&gt;VirtualBox提供虚拟机运行环境，Vargrant提供虚拟机配置的复制和分离运行，类似于Docker的雏形。在安装Vargrant插件时，可能会出现删除VirtualBox的警告，可直接忽视。安装完成之后需要建立Vagrantfile，该文件用于指定虚拟机初始化内存、IP等配置信息。使用&lt;span class="lang:default decode:true  crayon-inline "&gt;vagrant up&lt;/span&gt; 命令启动vagrant虚拟机，使用&lt;span class="lang:default decode:true  crayon-inline "&gt;vagrant ssh&lt;/span&gt; 命令与vagrant虚拟机进行ssh通讯。&lt;/p&gt;
&lt;h2 id="appscale"&gt;安装Appscale服务&lt;/h2&gt;
&lt;p&gt;Appscale是类似于Google App Engine的一个分布式应用运行平台。目标搭建的服务分别部署于本机和虚拟机集群。&lt;/p&gt;&lt;/div&gt;</description><link>http://localhost:2368/nimbits-yi-ge-iotshu-ju-hui-ji-ping-tai/</link><guid isPermaLink="false">59abdd16f59f8d07cf681654</guid><category>物联网</category><dc:creator>zhaodaolimeng</dc:creator><pubDate>Mon, 04 May 2015 04:42:00 GMT</pubDate><content:encoded>&lt;div class="kg-card-markdown"&gt;&lt;h1 id="1"&gt;1. 简介&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/bsautner/com.nimbits"&gt;Nimbits&lt;/a&gt;是一个专门针对物联网设计的数据汇集平台。该平台使用嵌入的hsql作为数据存储，使用Java作为编程语言，运行于Web环境中。相对于现有的Google/IBM 等公司的用于IoT的云平台，该平台简单、灵活、高度可定制，可为小型IoT业务提供了一套完整的数据解决方案。本文将介绍如何在本地搭建Appscale集群，并于之上部署Nimbits。&lt;/p&gt;
&lt;h1 id="2"&gt;2. 运行环境搭建&lt;/h1&gt;
&lt;p&gt;需要先后安装Virtual Box、Vagrant、Appscale。安装流程解释请参照&lt;br&gt;
&lt;a href="http://www.appscale.com/faststart"&gt;fast-install&lt;/a&gt;和&lt;a href="https://github.com/AppScale/appscale/wiki/AppScale-on-VirtualBox"&gt;Appscale wiki&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id="virtualboxvagrant"&gt;安装VirtualBox和Vagrant&lt;/h2&gt;
&lt;p&gt;VirtualBox提供虚拟机运行环境，Vargrant提供虚拟机配置的复制和分离运行，类似于Docker的雏形。在安装Vargrant插件时，可能会出现删除VirtualBox的警告，可直接忽视。安装完成之后需要建立Vagrantfile，该文件用于指定虚拟机初始化内存、IP等配置信息。使用&lt;span class="lang:default decode:true  crayon-inline "&gt;vagrant up&lt;/span&gt; 命令启动vagrant虚拟机，使用&lt;span class="lang:default decode:true  crayon-inline "&gt;vagrant ssh&lt;/span&gt; 命令与vagrant虚拟机进行ssh通讯。&lt;/p&gt;
&lt;h2 id="appscale"&gt;安装Appscale服务&lt;/h2&gt;
&lt;p&gt;Appscale是类似于Google App Engine的一个分布式应用运行平台。目标搭建的服务分别部署于本机和虚拟机集群。首先，下载基于Ubuntu 12.04的&lt;a href="http://download.appscale.com/apps/AppScale%201.12.0%20VirtualBox%20Image"&gt;Appscale镜像&lt;/a&gt;，可使用wget和aria2c从备用地址加速镜像下载过程。该镜像已经包含集群环境。其次，在本地安装Appscale工具。&lt;/p&gt;
&lt;p&gt;使用命令&lt;code&gt;appscale init cluster&lt;/code&gt;启动集群，使用命令&lt;code&gt;appscale up&lt;/code&gt;启动控制服务。使用&lt;code&gt;appscale clean&lt;/code&gt;和&lt;code&gt;appscale down&lt;/code&gt;分别可清除部署和停止服务。启动服务时，可能出现如下问题：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;stacktrace : Traceback (most recent call last):
File &amp;quot;/usr/local/appscale-tools/bin/appscale&amp;quot;, line 57, in
appscale.up()
File &amp;quot;/usr/local/appscale-tools/bin/../lib/appscale.py&amp;quot;, line 250, in up
AppScaleTools.run_instances(options)
File &amp;quot;/usr/local/appscale-tools/bin/../lib/appscale_tools.py&amp;quot;, line 362, in run_instances
node_layout)
File &amp;quot;/usr/local/appscale-tools/bin/../lib/remote_helper.py&amp;quot;, line 202, in start_head_node
raise AppControllerException(message)
...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;根据这个讨论，关闭所有的Python、Java和Ruby程序可以修复该问题。&lt;/p&gt;
&lt;h2 id="nimbits"&gt;在集群环境下部署Nimbits&lt;/h2&gt;
&lt;p&gt;本地使用环境为Ubuntu 12.04、mysql 5.5、Java 7。启动之后可访问以下API进行测试，Nimbits会对应返回系统时间。&lt;a href="http://localhost:8080/nimbits/service/v2/time%E3%80%82"&gt;http://localhost:8080/nimbits/service/v2/time。&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="3nimbits"&gt;3. 实例：使用Nimbits当计数器&lt;/h1&gt;
&lt;p&gt;可使用js向服务器发送一个post命令，用来测试Nimbits功能。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-javascript"&gt;function updateCounter() {
    $.post(
        &amp;quot;http://cloud.nimbits.com//service/v2/value&amp;quot;,
        {
            email: &amp;quot;youremail@gmail.com&amp;quot;,
            key: &amp;quot;secret&amp;quot;,
            id:  &amp;quot;youremail@gmail.com/counter&amp;quot;,
            json:&amp;quot;{&amp;quot;d&amp;quot;:1.0}&amp;quot;
        },
        function(data){  }
    );
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;需要注意的是当使用POST方法时，Nimbits端对应节点的设置需要是Read/Write To Single Point。每次POST请求后，对应的节点&amp;quot;d&amp;quot;的数据会加1。&lt;/p&gt;
&lt;/div&gt;</content:encoded></item></channel></rss>